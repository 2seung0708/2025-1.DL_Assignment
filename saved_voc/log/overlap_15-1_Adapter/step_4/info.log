2025-05-22 12:29:14,649 - train(rank0) - INFO - overlap / 15-1 / step: 4
2025-05-22 12:29:14,650 - train(rank0) - INFO - The number of datasets: 500 / 84 / 1421
2025-05-22 12:29:14,650 - train(rank0) - INFO - Old Classes: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]
2025-05-22 12:29:14,650 - train(rank0) - INFO - New Classes: [19]
2025-05-22 12:29:15,539 - train(rank0) - INFO - DeepLabV3(
  (backbone): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): SyncBatchNorm(512, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): SyncBatchNorm(1024, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (6): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (7): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (8): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (9): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (10): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (11): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (12): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (13): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (14): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (15): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (16): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (17): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (18): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (19): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (20): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (21): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (22): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(2048, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(2048, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
        (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(2048, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
        (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(2048, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
  )
  (aspp): ASPP(
    (convs): ModuleList(
      (0): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): ASPPConv(
        (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6), bias=False)
        (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): ASPPConv(
        (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12), bias=False)
        (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (3): ASPPConv(
        (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(18, 18), dilation=(18, 18), bias=False)
        (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (4): ASPPPooling(
        (0): AdaptiveAvgPool2d(output_size=1)
        (1): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU(inplace=True)
      )
    )
    (project): Sequential(
      (0): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Dropout(p=0.1, inplace=False)
    )
    (last_conv): Sequential(
      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
  )
  (cls): ModuleList(
    (0): Conv2d(256, 15, kernel_size=(1, 1), stride=(1, 1))
    (1): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))
    (2): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))
    (3): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))
    (4): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))
  )
)
2025-05-22 12:29:15,922 - train(rank0) - INFO - Load weights from a previous step:saved_voc/models/overlap_15-1_Adapter/step_3/checkpoint-epoch60.pth
2025-05-22 12:29:16,298 - train(rank0) - INFO - ** Random Initialization **
2025-05-22 12:29:19,041 - train(rank0) - INFO - pos_weight - 4
2025-05-22 12:29:19,041 - train(rank0) - INFO - Total loss = 1 * L_mbce + 5 * L_pkd
2025-05-22 12:29:19,041 - train(rank0) - INFO - computing number of pixels...
2025-05-22 12:29:24,547 - train(rank0) - INFO - [0/20]
2025-05-22 12:29:24,999 - train(rank0) - INFO - [4/20]
2025-05-22 12:29:25,444 - train(rank0) - INFO - [8/20]
2025-05-22 12:29:25,914 - train(rank0) - INFO - [12/20]
2025-05-22 12:29:26,478 - train(rank0) - INFO - [16/20]
2025-05-22 12:29:26,954 - train(rank0) - INFO - tensor([[90]])
2025-05-22 12:29:32,972 - train(rank2) - INFO - tensor([[90]])
2025-05-22 12:29:33,081 - train(rank1) - INFO - tensor([[90]])
2025-05-22 12:29:33,088 - train(rank0) - INFO - Epoch - 1
2025-05-22 12:29:33,088 - train(rank0) - INFO - computing pred number of pixels...
2025-05-22 12:29:39,847 - train(rank0) - INFO - [0/20]
2025-05-22 12:29:40,238 - train(rank0) - INFO - [4/20]
2025-05-22 12:29:40,720 - train(rank0) - INFO - [8/20]
2025-05-22 12:29:41,205 - train(rank0) - INFO - [12/20]
2025-05-22 12:29:41,693 - train(rank0) - INFO - [16/20]
2025-05-22 12:29:52,770 - train(rank0) - INFO - lr[0]: 0.000100 / lr[1]: 0.001000 / lr[2]: 0.001000
2025-05-22 12:29:52,771 - train(rank0) - INFO - [0/20]
2025-05-22 12:29:52,840 - torch.nn.parallel.distributed - INFO - Reducer buckets have been rebuilt in this iteration.
2025-05-22 12:29:52,845 - torch.nn.parallel.distributed - INFO - Reducer buckets have been rebuilt in this iteration.
2025-05-22 12:29:52,845 - torch.nn.parallel.distributed - INFO - Reducer buckets have been rebuilt in this iteration.
2025-05-22 12:29:55,165 - train(rank0) - INFO - [4/20]
2025-05-22 12:29:57,591 - train(rank0) - INFO - [8/20]
2025-05-22 12:30:00,031 - train(rank0) - INFO - [12/20]
2025-05-22 12:30:02,417 - train(rank0) - INFO - [16/20]
2025-05-22 12:30:04,601 - train(rank0) - INFO -     epoch          : 1
2025-05-22 12:30:04,602 - train(rank0) - INFO -     loss           : 0.35768414884805677
2025-05-22 12:30:04,602 - train(rank0) - INFO -     loss_mbce      : 0.21695910431444645
2025-05-22 12:30:04,603 - train(rank0) - INFO -     loss_pkd       : 0.011473596714495216
2025-05-22 12:30:04,603 - train(rank0) - INFO -     loss_cont      : 0.06495856523513793
2025-05-22 12:30:04,603 - train(rank0) - INFO -     loss_uncer     : 0.06429288148880005
2025-05-22 12:30:04,712 - train(rank0) - INFO - Epoch - 2
2025-05-22 12:30:10,758 - train(rank0) - INFO - lr[0]: 0.000098 / lr[1]: 0.000985 / lr[2]: 0.000985
2025-05-22 12:30:10,758 - train(rank0) - INFO - [0/20]
2025-05-22 12:30:13,180 - train(rank0) - INFO - [4/20]
2025-05-22 12:30:15,553 - train(rank0) - INFO - [8/20]
2025-05-22 12:30:17,946 - train(rank0) - INFO - [12/20]
2025-05-22 12:30:20,337 - train(rank0) - INFO - [16/20]
2025-05-22 12:30:22,558 - train(rank0) - INFO -     epoch          : 2
2025-05-22 12:30:22,559 - train(rank0) - INFO -     loss           : 0.23932725936174393
2025-05-22 12:30:22,559 - train(rank0) - INFO -     loss_mbce      : 0.0989205826073885
2025-05-22 12:30:22,559 - train(rank0) - INFO -     loss_pkd       : 0.0154342039313633
2025-05-22 12:30:22,560 - train(rank0) - INFO -     loss_cont      : 0.06298312455415725
2025-05-22 12:30:22,560 - train(rank0) - INFO -     loss_uncer     : 0.06198934674263
2025-05-22 12:30:22,570 - train(rank0) - INFO - Epoch - 3
2025-05-22 12:30:28,650 - train(rank0) - INFO - lr[0]: 0.000097 / lr[1]: 0.000970 / lr[2]: 0.000970
2025-05-22 12:30:28,651 - train(rank0) - INFO - [0/20]
2025-05-22 12:30:31,067 - train(rank0) - INFO - [4/20]
2025-05-22 12:30:33,506 - train(rank0) - INFO - [8/20]
2025-05-22 12:30:35,895 - train(rank0) - INFO - [12/20]
2025-05-22 12:30:38,285 - train(rank0) - INFO - [16/20]
2025-05-22 12:30:40,534 - train(rank0) - INFO -     epoch          : 3
2025-05-22 12:30:40,536 - train(rank0) - INFO -     loss           : 0.2240487076342106
2025-05-22 12:30:40,536 - train(rank0) - INFO -     loss_mbce      : 0.07983006313443183
2025-05-22 12:30:40,537 - train(rank0) - INFO -     loss_pkd       : 0.01938332340796478
2025-05-22 12:30:40,537 - train(rank0) - INFO -     loss_cont      : 0.06315819293260574
2025-05-22 12:30:40,537 - train(rank0) - INFO -     loss_uncer     : 0.06167712807655336
2025-05-22 12:30:40,546 - train(rank0) - INFO - Epoch - 4
2025-05-22 12:30:46,425 - train(rank0) - INFO - lr[0]: 0.000095 / lr[1]: 0.000955 / lr[2]: 0.000955
2025-05-22 12:30:46,426 - train(rank0) - INFO - [0/20]
2025-05-22 12:30:48,830 - train(rank0) - INFO - [4/20]
2025-05-22 12:30:51,239 - train(rank0) - INFO - [8/20]
2025-05-22 12:30:53,617 - train(rank0) - INFO - [12/20]
2025-05-22 12:30:56,020 - train(rank0) - INFO - [16/20]
2025-05-22 12:30:58,221 - train(rank0) - INFO -     epoch          : 4
2025-05-22 12:30:58,222 - train(rank0) - INFO -     loss           : 0.20761346593499183
2025-05-22 12:30:58,222 - train(rank0) - INFO -     loss_mbce      : 0.07029889393597841
2025-05-22 12:30:58,222 - train(rank0) - INFO -     loss_pkd       : 0.013394237408647314
2025-05-22 12:30:58,223 - train(rank0) - INFO -     loss_cont      : 0.062011856734752645
2025-05-22 12:30:58,223 - train(rank0) - INFO -     loss_uncer     : 0.06190847367048262
2025-05-22 12:30:58,241 - train(rank0) - INFO - Epoch - 5
2025-05-22 12:31:04,427 - train(rank0) - INFO - lr[0]: 0.000094 / lr[1]: 0.000940 / lr[2]: 0.000940
2025-05-22 12:31:04,427 - train(rank0) - INFO - [0/20]
2025-05-22 12:31:06,821 - train(rank0) - INFO - [4/20]
2025-05-22 12:31:09,201 - train(rank0) - INFO - [8/20]
2025-05-22 12:31:11,571 - train(rank0) - INFO - [12/20]
2025-05-22 12:31:13,962 - train(rank0) - INFO - [16/20]
2025-05-22 12:31:16,162 - train(rank0) - INFO -     epoch          : 5
2025-05-22 12:31:16,163 - train(rank0) - INFO -     loss           : 0.20620278194546698
2025-05-22 12:31:16,163 - train(rank0) - INFO -     loss_mbce      : 0.0747032357379794
2025-05-22 12:31:16,163 - train(rank0) - INFO -     loss_pkd       : 0.0075614959496306255
2025-05-22 12:31:16,163 - train(rank0) - INFO -     loss_cont      : 0.06286680907011032
2025-05-22 12:31:16,163 - train(rank0) - INFO -     loss_uncer     : 0.06107123970985412
2025-05-22 12:31:16,193 - train(rank0) - INFO - Epoch - 6
2025-05-22 12:31:22,373 - train(rank0) - INFO - lr[0]: 0.000092 / lr[1]: 0.000925 / lr[2]: 0.000925
2025-05-22 12:31:22,373 - train(rank0) - INFO - [0/20]
2025-05-22 12:31:24,766 - train(rank0) - INFO - [4/20]
2025-05-22 12:31:27,153 - train(rank0) - INFO - [8/20]
2025-05-22 12:31:29,562 - train(rank0) - INFO - [12/20]
2025-05-22 12:31:31,963 - train(rank0) - INFO - [16/20]
2025-05-22 12:31:34,145 - train(rank0) - INFO -     epoch          : 6
2025-05-22 12:31:34,146 - train(rank0) - INFO -     loss           : 0.19351150020956992
2025-05-22 12:31:34,146 - train(rank0) - INFO -     loss_mbce      : 0.060432739555835724
2025-05-22 12:31:34,146 - train(rank0) - INFO -     loss_pkd       : 0.009782443608855829
2025-05-22 12:31:34,146 - train(rank0) - INFO -     loss_cont      : 0.06045759320259094
2025-05-22 12:31:34,147 - train(rank0) - INFO -     loss_uncer     : 0.06283871829509736
2025-05-22 12:31:34,183 - train(rank0) - INFO - Epoch - 7
2025-05-22 12:31:40,235 - train(rank0) - INFO - lr[0]: 0.000091 / lr[1]: 0.000910 / lr[2]: 0.000910
2025-05-22 12:31:40,235 - train(rank0) - INFO - [0/20]
2025-05-22 12:31:42,593 - train(rank0) - INFO - [4/20]
2025-05-22 12:31:44,984 - train(rank0) - INFO - [8/20]
2025-05-22 12:31:47,371 - train(rank0) - INFO - [12/20]
2025-05-22 12:31:49,742 - train(rank0) - INFO - [16/20]
2025-05-22 12:31:51,946 - train(rank0) - INFO -     epoch          : 7
2025-05-22 12:31:51,947 - train(rank0) - INFO -     loss           : 0.2015335500240326
2025-05-22 12:31:51,947 - train(rank0) - INFO -     loss_mbce      : 0.06995442043989897
2025-05-22 12:31:51,947 - train(rank0) - INFO -     loss_pkd       : 0.010415795957669616
2025-05-22 12:31:51,947 - train(rank0) - INFO -     loss_cont      : 0.06093625217676163
2025-05-22 12:31:51,947 - train(rank0) - INFO -     loss_uncer     : 0.060227079242467875
2025-05-22 12:31:52,017 - train(rank0) - INFO - Epoch - 8
2025-05-22 12:31:58,211 - train(rank0) - INFO - lr[0]: 0.000089 / lr[1]: 0.000894 / lr[2]: 0.000894
2025-05-22 12:31:58,211 - train(rank0) - INFO - [0/20]
2025-05-22 12:32:00,604 - train(rank0) - INFO - [4/20]
2025-05-22 12:32:02,967 - train(rank0) - INFO - [8/20]
2025-05-22 12:32:05,288 - train(rank0) - INFO - [12/20]
2025-05-22 12:32:07,648 - train(rank0) - INFO - [16/20]
2025-05-22 12:32:09,864 - train(rank0) - INFO -     epoch          : 8
2025-05-22 12:32:09,865 - train(rank0) - INFO -     loss           : 0.19174561351537706
2025-05-22 12:32:09,866 - train(rank0) - INFO -     loss_mbce      : 0.05725953318178654
2025-05-22 12:32:09,866 - train(rank0) - INFO -     loss_pkd       : 0.010206024613580666
2025-05-22 12:32:09,866 - train(rank0) - INFO -     loss_cont      : 0.06081051617860793
2025-05-22 12:32:09,866 - train(rank0) - INFO -     loss_uncer     : 0.06346953511238097
2025-05-22 12:32:09,892 - train(rank0) - INFO - Epoch - 9
2025-05-22 12:32:15,872 - train(rank0) - INFO - lr[0]: 0.000088 / lr[1]: 0.000879 / lr[2]: 0.000879
2025-05-22 12:32:15,872 - train(rank0) - INFO - [0/20]
2025-05-22 12:32:18,274 - train(rank0) - INFO - [4/20]
2025-05-22 12:32:20,625 - train(rank0) - INFO - [8/20]
2025-05-22 12:32:23,019 - train(rank0) - INFO - [12/20]
2025-05-22 12:32:25,442 - train(rank0) - INFO - [16/20]
2025-05-22 12:32:27,614 - train(rank0) - INFO -     epoch          : 9
2025-05-22 12:32:27,615 - train(rank0) - INFO -     loss           : 0.1957674540579319
2025-05-22 12:32:27,615 - train(rank0) - INFO -     loss_mbce      : 0.05806802734732628
2025-05-22 12:32:27,615 - train(rank0) - INFO -     loss_pkd       : 0.01133726658008527
2025-05-22 12:32:27,616 - train(rank0) - INFO -     loss_cont      : 0.06116376996040344
2025-05-22 12:32:27,616 - train(rank0) - INFO -     loss_uncer     : 0.06519838660955429
2025-05-22 12:32:27,645 - train(rank0) - INFO - Epoch - 10
2025-05-22 12:32:33,637 - train(rank0) - INFO - lr[0]: 0.000086 / lr[1]: 0.000864 / lr[2]: 0.000864
2025-05-22 12:32:33,638 - train(rank0) - INFO - [0/20]
2025-05-22 12:32:35,955 - train(rank0) - INFO - [4/20]
2025-05-22 12:32:38,302 - train(rank0) - INFO - [8/20]
2025-05-22 12:32:40,667 - train(rank0) - INFO - [12/20]
2025-05-22 12:32:43,043 - train(rank0) - INFO - [16/20]
2025-05-22 12:32:45,356 - train(rank0) - INFO - Number of val loader: 84
2025-05-22 12:32:50,520 - train(rank0) - INFO -     epoch          : 10
2025-05-22 12:32:50,521 - train(rank0) - INFO -     loss           : 0.19019201770424843
2025-05-22 12:32:50,521 - train(rank0) - INFO -     loss_mbce      : 0.05964647121727466
2025-05-22 12:32:50,521 - train(rank0) - INFO -     loss_pkd       : 0.009590482717612758
2025-05-22 12:32:50,522 - train(rank0) - INFO -     loss_cont      : 0.06004565447568892
2025-05-22 12:32:50,522 - train(rank0) - INFO -     loss_uncer     : 0.060909407138824465
2025-05-22 12:32:50,522 - train(rank0) - INFO -     val_Pixel_Accuracy_old: 97.70
2025-05-22 12:32:50,522 - train(rank0) - INFO -     val_Pixel_Accuracy_new: 78.30
2025-05-22 12:32:50,522 - train(rank0) - INFO -     val_Pixel_Accuracy_harmonic: 86.93
2025-05-22 12:32:50,522 - train(rank0) - INFO -     val_Pixel_Accuracy_overall: 91.30
2025-05-22 12:32:50,522 - train(rank0) - INFO -     val_Pixel_Accuracy_Class_old: 97.70
2025-05-22 12:32:50,522 - train(rank0) - INFO -     val_Pixel_Accuracy_Class_new: 78.30
2025-05-22 12:32:50,522 - train(rank0) - INFO -     val_Pixel_Accuracy_Class_harmonic: 86.93
2025-05-22 12:32:50,522 - train(rank0) - INFO -     val_Pixel_Accuracy_Class_overall: 88.00
2025-05-22 12:32:50,522 - train(rank0) - INFO -     val_Pixel_Accuracy_Class_by_class: 
 0  background 97.70
19 *train 78.30

2025-05-22 12:32:50,522 - train(rank0) - INFO -     val_Mean_Intersection_over_Union_old: 89.13
2025-05-22 12:32:50,523 - train(rank0) - INFO -     val_Mean_Intersection_over_Union_new: 77.46
2025-05-22 12:32:50,523 - train(rank0) - INFO -     val_Mean_Intersection_over_Union_harmonic: 82.89
2025-05-22 12:32:50,523 - train(rank0) - INFO -     val_Mean_Intersection_over_Union_overall: 83.30
2025-05-22 12:32:50,523 - train(rank0) - INFO -     val_Mean_Intersection_over_Union_by_class: 
 0  background 89.13
19 *train 77.46

2025-05-22 12:32:51,110 - train(rank0) - INFO - Saving checkpoint: saved_voc/models/overlap_15-1_Adapter/step_4/checkpoint-epoch60.pth ...
2025-05-22 12:32:51,111 - train(rank0) - INFO - computing prototypes...
2025-05-22 12:32:56,258 - train(rank0) - INFO - [0/20]
2025-05-22 12:32:56,764 - train(rank0) - INFO - [4/20]
2025-05-22 12:32:57,260 - train(rank0) - INFO - [8/20]
2025-05-22 12:32:57,750 - train(rank0) - INFO - [12/20]
2025-05-22 12:32:58,240 - train(rank0) - INFO - [16/20]
2025-05-22 12:32:58,966 - train(rank0) - INFO - computing noise...
2025-05-22 12:33:04,145 - train(rank0) - INFO - [0/20]
2025-05-22 12:33:04,652 - train(rank0) - INFO - [4/20]
2025-05-22 12:33:05,150 - train(rank0) - INFO - [8/20]
2025-05-22 12:33:05,647 - train(rank0) - INFO - [12/20]
2025-05-22 12:33:06,144 - train(rank0) - INFO - [16/20]
2025-05-22 12:33:06,854 - train(rank0) - INFO - Epoch - 11
2025-05-22 12:33:13,218 - train(rank0) - INFO - lr[0]: 0.000085 / lr[1]: 0.000849 / lr[2]: 0.000849
2025-05-22 12:33:13,218 - train(rank0) - INFO - [0/20]
2025-05-22 12:33:15,601 - train(rank0) - INFO - [4/20]
2025-05-22 12:33:17,945 - train(rank0) - INFO - [8/20]
2025-05-22 12:33:20,331 - train(rank0) - INFO - [12/20]
2025-05-22 12:33:22,682 - train(rank0) - INFO - [16/20]
2025-05-22 12:33:24,919 - train(rank0) - INFO -     epoch          : 11
2025-05-22 12:33:24,920 - train(rank0) - INFO -     loss           : 0.18952613919973374
2025-05-22 12:33:24,920 - train(rank0) - INFO -     loss_mbce      : 0.06109726894646883
2025-05-22 12:33:24,920 - train(rank0) - INFO -     loss_pkd       : 0.008140571910189465
2025-05-22 12:33:24,920 - train(rank0) - INFO -     loss_cont      : 0.059715809822082536
2025-05-22 12:33:24,921 - train(rank0) - INFO -     loss_uncer     : 0.06057248622179031
2025-05-22 12:33:24,929 - train(rank0) - INFO - Epoch - 12
2025-05-22 12:33:31,172 - train(rank0) - INFO - lr[0]: 0.000083 / lr[1]: 0.000833 / lr[2]: 0.000833
2025-05-22 12:33:31,173 - train(rank0) - INFO - [0/20]
2025-05-22 12:33:33,565 - train(rank0) - INFO - [4/20]
2025-05-22 12:33:35,918 - train(rank0) - INFO - [8/20]
2025-05-22 12:33:38,357 - train(rank0) - INFO - [12/20]
2025-05-22 12:33:40,727 - train(rank0) - INFO - [16/20]
2025-05-22 12:33:43,005 - train(rank0) - INFO -     epoch          : 12
2025-05-22 12:33:43,006 - train(rank0) - INFO -     loss           : 0.18490923196077347
2025-05-22 12:33:43,006 - train(rank0) - INFO -     loss_mbce      : 0.05495148664340377
2025-05-22 12:33:43,006 - train(rank0) - INFO -     loss_pkd       : 0.007121546848793514
2025-05-22 12:33:43,007 - train(rank0) - INFO -     loss_cont      : 0.0603506922721863
2025-05-22 12:33:43,007 - train(rank0) - INFO -     loss_uncer     : 0.06248550355434418
2025-05-22 12:33:43,016 - train(rank0) - INFO - Epoch - 13
2025-05-22 12:33:49,153 - train(rank0) - INFO - lr[0]: 0.000082 / lr[1]: 0.000818 / lr[2]: 0.000818
2025-05-22 12:33:49,153 - train(rank0) - INFO - [0/20]
2025-05-22 12:33:51,540 - train(rank0) - INFO - [4/20]
2025-05-22 12:33:53,948 - train(rank0) - INFO - [8/20]
2025-05-22 12:33:56,305 - train(rank0) - INFO - [12/20]
2025-05-22 12:33:58,696 - train(rank0) - INFO - [16/20]
2025-05-22 12:34:00,876 - train(rank0) - INFO -     epoch          : 13
2025-05-22 12:34:00,877 - train(rank0) - INFO -     loss           : 0.1896487794816494
2025-05-22 12:34:00,877 - train(rank0) - INFO -     loss_mbce      : 0.05388675099238753
2025-05-22 12:34:00,877 - train(rank0) - INFO -     loss_pkd       : 0.0146593412355287
2025-05-22 12:34:00,877 - train(rank0) - INFO -     loss_cont      : 0.059588571786880505
2025-05-22 12:34:00,877 - train(rank0) - INFO -     loss_uncer     : 0.06151411384344101
2025-05-22 12:34:00,905 - train(rank0) - INFO - Epoch - 14
2025-05-22 12:34:07,036 - train(rank0) - INFO - lr[0]: 0.000080 / lr[1]: 0.000803 / lr[2]: 0.000803
2025-05-22 12:34:07,036 - train(rank0) - INFO - [0/20]
2025-05-22 12:34:09,423 - train(rank0) - INFO - [4/20]
2025-05-22 12:34:11,753 - train(rank0) - INFO - [8/20]
2025-05-22 12:34:14,170 - train(rank0) - INFO - [12/20]
2025-05-22 12:34:16,552 - train(rank0) - INFO - [16/20]
2025-05-22 12:34:18,767 - train(rank0) - INFO -     epoch          : 14
2025-05-22 12:34:18,768 - train(rank0) - INFO -     loss           : 0.18959901332855225
2025-05-22 12:34:18,768 - train(rank0) - INFO -     loss_mbce      : 0.06200670637190342
2025-05-22 12:34:18,768 - train(rank0) - INFO -     loss_pkd       : 0.0074913889257004485
2025-05-22 12:34:18,769 - train(rank0) - INFO -     loss_cont      : 0.060089250802993766
2025-05-22 12:34:18,769 - train(rank0) - INFO -     loss_uncer     : 0.06001166313886643
2025-05-22 12:34:18,798 - train(rank0) - INFO - Epoch - 15
2025-05-22 12:34:25,125 - train(rank0) - INFO - lr[0]: 0.000079 / lr[1]: 0.000787 / lr[2]: 0.000787
2025-05-22 12:34:25,126 - train(rank0) - INFO - [0/20]
2025-05-22 12:34:27,531 - train(rank0) - INFO - [4/20]
2025-05-22 12:34:29,923 - train(rank0) - INFO - [8/20]
2025-05-22 12:34:32,302 - train(rank0) - INFO - [12/20]
2025-05-22 12:34:34,665 - train(rank0) - INFO - [16/20]
2025-05-22 12:34:36,923 - train(rank0) - INFO -     epoch          : 15
2025-05-22 12:34:36,923 - train(rank0) - INFO -     loss           : 0.18557738289237022
2025-05-22 12:34:36,923 - train(rank0) - INFO -     loss_mbce      : 0.0555287504568696
2025-05-22 12:34:36,924 - train(rank0) - INFO -     loss_pkd       : 0.007438318010827061
2025-05-22 12:34:36,924 - train(rank0) - INFO -     loss_cont      : 0.05994931340217591
2025-05-22 12:34:36,924 - train(rank0) - INFO -     loss_uncer     : 0.06266099750995638
2025-05-22 12:34:36,930 - train(rank0) - INFO - Epoch - 16
2025-05-22 12:34:43,770 - train(rank0) - INFO - lr[0]: 0.000077 / lr[1]: 0.000772 / lr[2]: 0.000772
2025-05-22 12:34:43,770 - train(rank0) - INFO - [0/20]
2025-05-22 12:34:46,172 - train(rank0) - INFO - [4/20]
2025-05-22 12:34:48,539 - train(rank0) - INFO - [8/20]
2025-05-22 12:34:50,936 - train(rank0) - INFO - [12/20]
2025-05-22 12:34:53,300 - train(rank0) - INFO - [16/20]
2025-05-22 12:34:55,561 - train(rank0) - INFO -     epoch          : 16
2025-05-22 12:34:55,562 - train(rank0) - INFO -     loss           : 0.18142216503620148
2025-05-22 12:34:55,562 - train(rank0) - INFO -     loss_mbce      : 0.053468201868236066
2025-05-22 12:34:55,562 - train(rank0) - INFO -     loss_pkd       : 0.007285091342055239
2025-05-22 12:34:55,563 - train(rank0) - INFO -     loss_cont      : 0.058871356844902044
2025-05-22 12:34:55,563 - train(rank0) - INFO -     loss_uncer     : 0.06179751306772232
2025-05-22 12:34:55,572 - train(rank0) - INFO - Epoch - 17
2025-05-22 12:35:02,595 - train(rank0) - INFO - lr[0]: 0.000076 / lr[1]: 0.000756 / lr[2]: 0.000756
2025-05-22 12:35:02,595 - train(rank0) - INFO - [0/20]
2025-05-22 12:35:05,087 - train(rank0) - INFO - [4/20]
2025-05-22 12:35:07,419 - train(rank0) - INFO - [8/20]
2025-05-22 12:35:09,805 - train(rank0) - INFO - [12/20]
2025-05-22 12:35:12,149 - train(rank0) - INFO - [16/20]
2025-05-22 12:35:14,361 - train(rank0) - INFO -     epoch          : 17
2025-05-22 12:35:14,362 - train(rank0) - INFO -     loss           : 0.1797717273235321
2025-05-22 12:35:14,363 - train(rank0) - INFO -     loss_mbce      : 0.05037736231461167
2025-05-22 12:35:14,363 - train(rank0) - INFO -     loss_pkd       : 0.00722413879702799
2025-05-22 12:35:14,363 - train(rank0) - INFO -     loss_cont      : 0.0594718986749649
2025-05-22 12:35:14,363 - train(rank0) - INFO -     loss_uncer     : 0.06269832551479339
2025-05-22 12:35:14,407 - train(rank0) - INFO - Epoch - 18
2025-05-22 12:35:20,531 - train(rank0) - INFO - lr[0]: 0.000074 / lr[1]: 0.000741 / lr[2]: 0.000741
2025-05-22 12:35:20,532 - train(rank0) - INFO - [0/20]
2025-05-22 12:35:22,920 - train(rank0) - INFO - [4/20]
2025-05-22 12:35:25,276 - train(rank0) - INFO - [8/20]
2025-05-22 12:35:27,661 - train(rank0) - INFO - [12/20]
2025-05-22 12:35:30,054 - train(rank0) - INFO - [16/20]
2025-05-22 12:35:32,254 - train(rank0) - INFO -     epoch          : 18
2025-05-22 12:35:32,255 - train(rank0) - INFO -     loss           : 0.17511082217097282
2025-05-22 12:35:32,255 - train(rank0) - INFO -     loss_mbce      : 0.048009227681905034
2025-05-22 12:35:32,256 - train(rank0) - INFO -     loss_pkd       : 0.005310769993229769
2025-05-22 12:35:32,256 - train(rank0) - INFO -     loss_cont      : 0.058333643972873696
2025-05-22 12:35:32,256 - train(rank0) - INFO -     loss_uncer     : 0.06345717906951903
2025-05-22 12:35:32,267 - train(rank0) - INFO - Epoch - 19
2025-05-22 12:35:37,991 - train(rank0) - INFO - lr[0]: 0.000073 / lr[1]: 0.000725 / lr[2]: 0.000725
2025-05-22 12:35:37,991 - train(rank0) - INFO - [0/20]
2025-05-22 12:35:40,389 - train(rank0) - INFO - [4/20]
2025-05-22 12:35:42,763 - train(rank0) - INFO - [8/20]
2025-05-22 12:35:45,145 - train(rank0) - INFO - [12/20]
2025-05-22 12:35:47,491 - train(rank0) - INFO - [16/20]
2025-05-22 12:35:49,653 - train(rank0) - INFO -     epoch          : 19
2025-05-22 12:35:49,654 - train(rank0) - INFO -     loss           : 0.17574879825115203
2025-05-22 12:35:49,654 - train(rank0) - INFO -     loss_mbce      : 0.0461132469587028
2025-05-22 12:35:49,655 - train(rank0) - INFO -     loss_pkd       : 0.007599056887556799
2025-05-22 12:35:49,655 - train(rank0) - INFO -     loss_cont      : 0.058421023190021515
2025-05-22 12:35:49,655 - train(rank0) - INFO -     loss_uncer     : 0.0636154718697071
2025-05-22 12:35:49,670 - train(rank0) - INFO - Epoch - 20
2025-05-22 12:35:55,486 - train(rank0) - INFO - lr[0]: 0.000071 / lr[1]: 0.000710 / lr[2]: 0.000710
2025-05-22 12:35:55,486 - train(rank0) - INFO - [0/20]
2025-05-22 12:35:57,868 - train(rank0) - INFO - [4/20]
2025-05-22 12:36:00,268 - train(rank0) - INFO - [8/20]
2025-05-22 12:36:02,599 - train(rank0) - INFO - [12/20]
2025-05-22 12:36:04,978 - train(rank0) - INFO - [16/20]
2025-05-22 12:36:07,193 - train(rank0) - INFO - Number of val loader: 84
2025-05-22 12:36:11,395 - train(rank0) - INFO -     epoch          : 20
2025-05-22 12:36:11,396 - train(rank0) - INFO -     loss           : 0.17886373922228813
2025-05-22 12:36:11,396 - train(rank0) - INFO -     loss_mbce      : 0.05042231110855937
2025-05-22 12:36:11,396 - train(rank0) - INFO -     loss_pkd       : 0.006332626144285314
2025-05-22 12:36:11,396 - train(rank0) - INFO -     loss_cont      : 0.058837808072566986
2025-05-22 12:36:11,396 - train(rank0) - INFO -     loss_uncer     : 0.06327099129557609
2025-05-22 12:36:11,396 - train(rank0) - INFO -     val_Pixel_Accuracy_old: 97.70
2025-05-22 12:36:11,396 - train(rank0) - INFO -     val_Pixel_Accuracy_new: 79.28
2025-05-22 12:36:11,396 - train(rank0) - INFO -     val_Pixel_Accuracy_harmonic: 87.53
2025-05-22 12:36:11,396 - train(rank0) - INFO -     val_Pixel_Accuracy_overall: 91.62
2025-05-22 12:36:11,397 - train(rank0) - INFO -     val_Pixel_Accuracy_Class_old: 97.70
2025-05-22 12:36:11,397 - train(rank0) - INFO -     val_Pixel_Accuracy_Class_new: 79.28
2025-05-22 12:36:11,397 - train(rank0) - INFO -     val_Pixel_Accuracy_Class_harmonic: 87.53
2025-05-22 12:36:11,397 - train(rank0) - INFO -     val_Pixel_Accuracy_Class_overall: 88.49
2025-05-22 12:36:11,397 - train(rank0) - INFO -     val_Pixel_Accuracy_Class_by_class: 
 0  background 97.70
19 *train 79.28

2025-05-22 12:36:11,397 - train(rank0) - INFO -     val_Mean_Intersection_over_Union_old: 89.52
2025-05-22 12:36:11,397 - train(rank0) - INFO -     val_Mean_Intersection_over_Union_new: 78.45
2025-05-22 12:36:11,397 - train(rank0) - INFO -     val_Mean_Intersection_over_Union_harmonic: 83.62
2025-05-22 12:36:11,397 - train(rank0) - INFO -     val_Mean_Intersection_over_Union_overall: 83.98
2025-05-22 12:36:11,397 - train(rank0) - INFO -     val_Mean_Intersection_over_Union_by_class: 
 0  background 89.52
19 *train 78.45

2025-05-22 12:36:12,094 - train(rank0) - INFO - Saving checkpoint: saved_voc/models/overlap_15-1_Adapter/step_4/checkpoint-epoch60.pth ...
2025-05-22 12:36:12,094 - train(rank0) - INFO - computing prototypes...
2025-05-22 12:36:16,980 - train(rank0) - INFO - [0/20]
2025-05-22 12:36:17,482 - train(rank0) - INFO - [4/20]
2025-05-22 12:36:17,975 - train(rank0) - INFO - [8/20]
2025-05-22 12:36:18,465 - train(rank0) - INFO - [12/20]
2025-05-22 12:36:18,954 - train(rank0) - INFO - [16/20]
2025-05-22 12:36:19,607 - train(rank0) - INFO - computing noise...
2025-05-22 12:36:24,270 - train(rank0) - INFO - [0/20]
2025-05-22 12:36:24,773 - train(rank0) - INFO - [4/20]
2025-05-22 12:36:25,277 - train(rank0) - INFO - [8/20]
2025-05-22 12:36:25,774 - train(rank0) - INFO - [12/20]
2025-05-22 12:36:26,270 - train(rank0) - INFO - [16/20]
2025-05-22 12:36:27,013 - train(rank0) - INFO - Epoch - 21
2025-05-22 12:36:32,492 - train(rank0) - INFO - lr[0]: 0.000069 / lr[1]: 0.000694 / lr[2]: 0.000694
2025-05-22 12:36:32,492 - train(rank0) - INFO - [0/20]
2025-05-22 12:36:34,872 - train(rank0) - INFO - [4/20]
2025-05-22 12:36:37,217 - train(rank0) - INFO - [8/20]
2025-05-22 12:36:39,570 - train(rank0) - INFO - [12/20]
2025-05-22 12:36:41,895 - train(rank0) - INFO - [16/20]
2025-05-22 12:36:44,124 - train(rank0) - INFO -     epoch          : 21
2025-05-22 12:36:44,125 - train(rank0) - INFO -     loss           : 0.1874481186270714
2025-05-22 12:36:44,126 - train(rank0) - INFO -     loss_mbce      : 0.056803581304848194
2025-05-22 12:36:44,126 - train(rank0) - INFO -     loss_pkd       : 0.010305731018888764
2025-05-22 12:36:44,126 - train(rank0) - INFO -     loss_cont      : 0.05873034477233885
2025-05-22 12:36:44,127 - train(rank0) - INFO -     loss_uncer     : 0.06160845801234246
2025-05-22 12:36:44,143 - train(rank0) - INFO - Epoch - 22
2025-05-22 12:36:49,717 - train(rank0) - INFO - lr[0]: 0.000068 / lr[1]: 0.000679 / lr[2]: 0.000679
2025-05-22 12:36:49,717 - train(rank0) - INFO - [0/20]
2025-05-22 12:36:52,097 - train(rank0) - INFO - [4/20]
2025-05-22 12:36:54,461 - train(rank0) - INFO - [8/20]
2025-05-22 12:36:56,852 - train(rank0) - INFO - [12/20]
2025-05-22 12:36:59,187 - train(rank0) - INFO - [16/20]
2025-05-22 12:37:01,435 - train(rank0) - INFO -     epoch          : 22
2025-05-22 12:37:01,436 - train(rank0) - INFO -     loss           : 0.1805115297436714
2025-05-22 12:37:01,436 - train(rank0) - INFO -     loss_mbce      : 0.050430734269320966
2025-05-22 12:37:01,436 - train(rank0) - INFO -     loss_pkd       : 0.009944628181983717
2025-05-22 12:37:01,436 - train(rank0) - INFO -     loss_cont      : 0.05813539236783981
2025-05-22 12:37:01,436 - train(rank0) - INFO -     loss_uncer     : 0.06200077384710313
2025-05-22 12:37:01,443 - train(rank0) - INFO - Epoch - 23
2025-05-22 12:37:07,574 - train(rank0) - INFO - lr[0]: 0.000066 / lr[1]: 0.000663 / lr[2]: 0.000663
2025-05-22 12:37:07,574 - train(rank0) - INFO - [0/20]
2025-05-22 12:37:09,947 - train(rank0) - INFO - [4/20]
2025-05-22 12:37:12,300 - train(rank0) - INFO - [8/20]
2025-05-22 12:37:14,673 - train(rank0) - INFO - [12/20]
2025-05-22 12:37:17,018 - train(rank0) - INFO - [16/20]
2025-05-22 12:37:19,207 - train(rank0) - INFO -     epoch          : 23
2025-05-22 12:37:19,208 - train(rank0) - INFO -     loss           : 0.18397633656859397
2025-05-22 12:37:19,208 - train(rank0) - INFO -     loss_mbce      : 0.05197480171918869
2025-05-22 12:37:19,208 - train(rank0) - INFO -     loss_pkd       : 0.008828487450955436
2025-05-22 12:37:19,208 - train(rank0) - INFO -     loss_cont      : 0.05923568278551102
2025-05-22 12:37:19,209 - train(rank0) - INFO -     loss_uncer     : 0.06393736362457275
2025-05-22 12:37:19,328 - train(rank0) - INFO - Epoch - 24
2025-05-22 12:37:25,201 - train(rank0) - INFO - lr[0]: 0.000065 / lr[1]: 0.000647 / lr[2]: 0.000647
2025-05-22 12:37:25,201 - train(rank0) - INFO - [0/20]
2025-05-22 12:37:27,625 - train(rank0) - INFO - [4/20]
2025-05-22 12:37:29,983 - train(rank0) - INFO - [8/20]
2025-05-22 12:37:32,303 - train(rank0) - INFO - [12/20]
2025-05-22 12:37:34,719 - train(rank0) - INFO - [16/20]
2025-05-22 12:37:36,944 - train(rank0) - INFO -     epoch          : 24
2025-05-22 12:37:36,945 - train(rank0) - INFO -     loss           : 0.1772455431520939
2025-05-22 12:37:36,945 - train(rank0) - INFO -     loss_mbce      : 0.051385251060128215
2025-05-22 12:37:36,945 - train(rank0) - INFO -     loss_pkd       : 0.007418570756271947
2025-05-22 12:37:36,946 - train(rank0) - INFO -     loss_cont      : 0.058433523774147025
2025-05-22 12:37:36,946 - train(rank0) - INFO -     loss_uncer     : 0.06000819727778435
2025-05-22 12:37:37,017 - train(rank0) - INFO - Epoch - 25
2025-05-22 12:37:43,050 - train(rank0) - INFO - lr[0]: 0.000063 / lr[1]: 0.000631 / lr[2]: 0.000631
2025-05-22 12:37:43,050 - train(rank0) - INFO - [0/20]
2025-05-22 12:37:45,408 - train(rank0) - INFO - [4/20]
2025-05-22 12:37:47,847 - train(rank0) - INFO - [8/20]
2025-05-22 12:37:50,237 - train(rank0) - INFO - [12/20]
2025-05-22 12:37:52,581 - train(rank0) - INFO - [16/20]
2025-05-22 12:37:54,741 - train(rank0) - INFO -     epoch          : 25
2025-05-22 12:37:54,742 - train(rank0) - INFO -     loss           : 0.18107258081436156
2025-05-22 12:37:54,742 - train(rank0) - INFO -     loss_mbce      : 0.045909789577126506
2025-05-22 12:37:54,742 - train(rank0) - INFO -     loss_pkd       : 0.011835774574137758
2025-05-22 12:37:54,742 - train(rank0) - INFO -     loss_cont      : 0.05901290535926819
2025-05-22 12:37:54,742 - train(rank0) - INFO -     loss_uncer     : 0.06431410819292069
2025-05-22 12:37:54,757 - train(rank0) - INFO - Epoch - 26
2025-05-22 12:38:00,741 - train(rank0) - INFO - lr[0]: 0.000062 / lr[1]: 0.000616 / lr[2]: 0.000616
2025-05-22 12:38:00,741 - train(rank0) - INFO - [0/20]
2025-05-22 12:38:03,146 - train(rank0) - INFO - [4/20]
2025-05-22 12:38:05,500 - train(rank0) - INFO - [8/20]
2025-05-22 12:38:07,877 - train(rank0) - INFO - [12/20]
2025-05-22 12:38:10,273 - train(rank0) - INFO - [16/20]
2025-05-22 12:38:12,469 - train(rank0) - INFO -     epoch          : 26
2025-05-22 12:38:12,471 - train(rank0) - INFO -     loss           : 0.17387957870960236
2025-05-22 12:38:12,471 - train(rank0) - INFO -     loss_mbce      : 0.04694713717326522
2025-05-22 12:38:12,472 - train(rank0) - INFO -     loss_pkd       : 0.005396204782300629
2025-05-22 12:38:12,472 - train(rank0) - INFO -     loss_cont      : 0.05756631761789323
2025-05-22 12:38:12,472 - train(rank0) - INFO -     loss_uncer     : 0.0639699187874794
2025-05-22 12:38:12,518 - train(rank0) - INFO - Epoch - 27
2025-05-22 12:38:18,442 - train(rank0) - INFO - lr[0]: 0.000060 / lr[1]: 0.000600 / lr[2]: 0.000600
2025-05-22 12:38:18,442 - train(rank0) - INFO - [0/20]
2025-05-22 12:38:20,853 - train(rank0) - INFO - [4/20]
2025-05-22 12:38:23,195 - train(rank0) - INFO - [8/20]
2025-05-22 12:38:25,561 - train(rank0) - INFO - [12/20]
2025-05-22 12:38:27,901 - train(rank0) - INFO - [16/20]
2025-05-22 12:38:30,075 - train(rank0) - INFO -     epoch          : 27
2025-05-22 12:38:30,076 - train(rank0) - INFO -     loss           : 0.178274704515934
2025-05-22 12:38:30,076 - train(rank0) - INFO -     loss_mbce      : 0.05370974354445934
2025-05-22 12:38:30,076 - train(rank0) - INFO -     loss_pkd       : 0.006810562175814994
2025-05-22 12:38:30,076 - train(rank0) - INFO -     loss_cont      : 0.05795980989933014
2025-05-22 12:38:30,076 - train(rank0) - INFO -     loss_uncer     : 0.05979458943009376
2025-05-22 12:38:30,087 - train(rank0) - INFO - Epoch - 28
2025-05-22 12:38:35,881 - train(rank0) - INFO - lr[0]: 0.000058 / lr[1]: 0.000584 / lr[2]: 0.000584
2025-05-22 12:38:35,881 - train(rank0) - INFO - [0/20]
2025-05-22 12:38:38,253 - train(rank0) - INFO - [4/20]
2025-05-22 12:38:40,603 - train(rank0) - INFO - [8/20]
2025-05-22 12:38:43,008 - train(rank0) - INFO - [12/20]
2025-05-22 12:38:45,319 - train(rank0) - INFO - [16/20]
2025-05-22 12:38:47,473 - train(rank0) - INFO -     epoch          : 28
2025-05-22 12:38:47,474 - train(rank0) - INFO -     loss           : 0.17457715272903443
2025-05-22 12:38:47,477 - train(rank0) - INFO -     loss_mbce      : 0.04723416939377785
2025-05-22 12:38:47,478 - train(rank0) - INFO -     loss_pkd       : 0.005900721735088155
2025-05-22 12:38:47,478 - train(rank0) - INFO -     loss_cont      : 0.05814618200063706
2025-05-22 12:38:47,478 - train(rank0) - INFO -     loss_uncer     : 0.06329607725143435
2025-05-22 12:38:47,510 - train(rank0) - INFO - Epoch - 29
2025-05-22 12:38:53,467 - train(rank0) - INFO - lr[0]: 0.000057 / lr[1]: 0.000568 / lr[2]: 0.000568
2025-05-22 12:38:53,467 - train(rank0) - INFO - [0/20]
2025-05-22 12:38:55,889 - train(rank0) - INFO - [4/20]
2025-05-22 12:38:58,254 - train(rank0) - INFO - [8/20]
2025-05-22 12:39:00,613 - train(rank0) - INFO - [12/20]
2025-05-22 12:39:02,968 - train(rank0) - INFO - [16/20]
2025-05-22 12:39:05,279 - train(rank0) - INFO -     epoch          : 29
2025-05-22 12:39:05,280 - train(rank0) - INFO -     loss           : 0.17896758615970612
2025-05-22 12:39:05,280 - train(rank0) - INFO -     loss_mbce      : 0.051486224960535766
2025-05-22 12:39:05,280 - train(rank0) - INFO -     loss_pkd       : 0.007631555126863532
2025-05-22 12:39:05,281 - train(rank0) - INFO -     loss_cont      : 0.0584177455306053
2025-05-22 12:39:05,281 - train(rank0) - INFO -     loss_uncer     : 0.06143205806612968
2025-05-22 12:39:05,289 - train(rank0) - INFO - Epoch - 30
2025-05-22 12:39:11,362 - train(rank0) - INFO - lr[0]: 0.000055 / lr[1]: 0.000552 / lr[2]: 0.000552
2025-05-22 12:39:11,363 - train(rank0) - INFO - [0/20]
2025-05-22 12:39:13,762 - train(rank0) - INFO - [4/20]
2025-05-22 12:39:16,161 - train(rank0) - INFO - [8/20]
2025-05-22 12:39:18,584 - train(rank0) - INFO - [12/20]
2025-05-22 12:39:20,890 - train(rank0) - INFO - [16/20]
2025-05-22 12:39:23,118 - train(rank0) - INFO - Number of val loader: 84
2025-05-22 12:39:27,393 - train(rank0) - INFO -     epoch          : 30
2025-05-22 12:39:27,393 - train(rank0) - INFO -     loss           : 0.17600590512156486
2025-05-22 12:39:27,393 - train(rank0) - INFO -     loss_mbce      : 0.05100154420360923
2025-05-22 12:39:27,393 - train(rank0) - INFO -     loss_pkd       : 0.004676278913393617
2025-05-22 12:39:27,394 - train(rank0) - INFO -     loss_cont      : 0.05846784025430679
2025-05-22 12:39:27,394 - train(rank0) - INFO -     loss_uncer     : 0.061860238909721366
2025-05-22 12:39:27,394 - train(rank0) - INFO -     val_Pixel_Accuracy_old: 97.70
2025-05-22 12:39:27,394 - train(rank0) - INFO -     val_Pixel_Accuracy_new: 80.32
2025-05-22 12:39:27,394 - train(rank0) - INFO -     val_Pixel_Accuracy_harmonic: 88.16
2025-05-22 12:39:27,394 - train(rank0) - INFO -     val_Pixel_Accuracy_overall: 91.96
2025-05-22 12:39:27,394 - train(rank0) - INFO -     val_Pixel_Accuracy_Class_old: 97.70
2025-05-22 12:39:27,394 - train(rank0) - INFO -     val_Pixel_Accuracy_Class_new: 80.32
2025-05-22 12:39:27,394 - train(rank0) - INFO -     val_Pixel_Accuracy_Class_harmonic: 88.16
2025-05-22 12:39:27,394 - train(rank0) - INFO -     val_Pixel_Accuracy_Class_overall: 89.01
2025-05-22 12:39:27,394 - train(rank0) - INFO -     val_Pixel_Accuracy_Class_by_class: 
 0  background 97.70
19 *train 80.32

2025-05-22 12:39:27,394 - train(rank0) - INFO -     val_Mean_Intersection_over_Union_old: 89.90
2025-05-22 12:39:27,394 - train(rank0) - INFO -     val_Mean_Intersection_over_Union_new: 79.47
2025-05-22 12:39:27,394 - train(rank0) - INFO -     val_Mean_Intersection_over_Union_harmonic: 84.37
2025-05-22 12:39:27,394 - train(rank0) - INFO -     val_Mean_Intersection_over_Union_overall: 84.69
2025-05-22 12:39:27,394 - train(rank0) - INFO -     val_Mean_Intersection_over_Union_by_class: 
 0  background 89.90
19 *train 79.47

2025-05-22 12:39:28,057 - train(rank0) - INFO - Saving checkpoint: saved_voc/models/overlap_15-1_Adapter/step_4/checkpoint-epoch60.pth ...
2025-05-22 12:39:28,057 - train(rank0) - INFO - computing prototypes...
2025-05-22 12:39:32,859 - train(rank0) - INFO - [0/20]
2025-05-22 12:39:33,359 - train(rank0) - INFO - [4/20]
2025-05-22 12:39:33,851 - train(rank0) - INFO - [8/20]
2025-05-22 12:39:34,341 - train(rank0) - INFO - [12/20]
2025-05-22 12:39:34,832 - train(rank0) - INFO - [16/20]
2025-05-22 12:39:35,498 - train(rank0) - INFO - computing noise...
2025-05-22 12:39:40,189 - train(rank0) - INFO - [0/20]
2025-05-22 12:39:40,689 - train(rank0) - INFO - [4/20]
2025-05-22 12:39:41,186 - train(rank0) - INFO - [8/20]
2025-05-22 12:39:41,684 - train(rank0) - INFO - [12/20]
2025-05-22 12:39:42,180 - train(rank0) - INFO - [16/20]
2025-05-22 12:39:42,942 - train(rank0) - INFO - Epoch - 31
2025-05-22 12:39:48,790 - train(rank0) - INFO - lr[0]: 0.000054 / lr[1]: 0.000536 / lr[2]: 0.000536
2025-05-22 12:39:48,790 - train(rank0) - INFO - [0/20]
2025-05-22 12:39:51,193 - train(rank0) - INFO - [4/20]
2025-05-22 12:39:53,511 - train(rank0) - INFO - [8/20]
2025-05-22 12:39:55,904 - train(rank0) - INFO - [12/20]
2025-05-22 12:39:58,322 - train(rank0) - INFO - [16/20]
2025-05-22 12:40:00,479 - train(rank0) - INFO -     epoch          : 31
2025-05-22 12:40:00,479 - train(rank0) - INFO -     loss           : 0.1696521207690239
2025-05-22 12:40:00,480 - train(rank0) - INFO -     loss_mbce      : 0.04174894960597157
2025-05-22 12:40:00,480 - train(rank0) - INFO -     loss_pkd       : 0.0050371851393720135
2025-05-22 12:40:00,480 - train(rank0) - INFO -     loss_cont      : 0.05717760771512986
2025-05-22 12:40:00,480 - train(rank0) - INFO -     loss_uncer     : 0.0656883755326271
2025-05-22 12:40:00,538 - train(rank0) - INFO - Epoch - 32
2025-05-22 12:40:06,247 - train(rank0) - INFO - lr[0]: 0.000052 / lr[1]: 0.000520 / lr[2]: 0.000520
2025-05-22 12:40:06,248 - train(rank0) - INFO - [0/20]
2025-05-22 12:40:08,600 - train(rank0) - INFO - [4/20]
2025-05-22 12:40:10,961 - train(rank0) - INFO - [8/20]
2025-05-22 12:40:13,302 - train(rank0) - INFO - [12/20]
2025-05-22 12:40:15,706 - train(rank0) - INFO - [16/20]
2025-05-22 12:40:17,942 - train(rank0) - INFO -     epoch          : 32
2025-05-22 12:40:17,943 - train(rank0) - INFO -     loss           : 0.17427301183342933
2025-05-22 12:40:17,944 - train(rank0) - INFO -     loss_mbce      : 0.046082033589482305
2025-05-22 12:40:17,944 - train(rank0) - INFO -     loss_pkd       : 0.008387432499148417
2025-05-22 12:40:17,944 - train(rank0) - INFO -     loss_cont      : 0.05760074436664582
2025-05-22 12:40:17,944 - train(rank0) - INFO -     loss_uncer     : 0.06220279902219772
2025-05-22 12:40:17,950 - train(rank0) - INFO - Epoch - 33
2025-05-22 12:40:23,732 - train(rank0) - INFO - lr[0]: 0.000050 / lr[1]: 0.000504 / lr[2]: 0.000504
2025-05-22 12:40:23,732 - train(rank0) - INFO - [0/20]
2025-05-22 12:40:26,104 - train(rank0) - INFO - [4/20]
2025-05-22 12:40:28,464 - train(rank0) - INFO - [8/20]
2025-05-22 12:40:30,750 - train(rank0) - INFO - [12/20]
2025-05-22 12:40:33,076 - train(rank0) - INFO - [16/20]
2025-05-22 12:40:35,258 - train(rank0) - INFO -     epoch          : 33
2025-05-22 12:40:35,259 - train(rank0) - INFO -     loss           : 0.1725039876997471
2025-05-22 12:40:35,259 - train(rank0) - INFO -     loss_mbce      : 0.043085131142288444
2025-05-22 12:40:35,259 - train(rank0) - INFO -     loss_pkd       : 0.0074362633167766035
2025-05-22 12:40:35,260 - train(rank0) - INFO -     loss_cont      : 0.05724437087774278
2025-05-22 12:40:35,260 - train(rank0) - INFO -     loss_uncer     : 0.06473822176456454
2025-05-22 12:40:35,322 - train(rank0) - INFO - Epoch - 34
2025-05-22 12:40:41,393 - train(rank0) - INFO - lr[0]: 0.000049 / lr[1]: 0.000487 / lr[2]: 0.000487
2025-05-22 12:40:41,393 - train(rank0) - INFO - [0/20]
2025-05-22 12:40:43,762 - train(rank0) - INFO - [4/20]
2025-05-22 12:40:46,122 - train(rank0) - INFO - [8/20]
2025-05-22 12:40:48,418 - train(rank0) - INFO - [12/20]
2025-05-22 12:40:50,802 - train(rank0) - INFO - [16/20]
2025-05-22 12:40:52,904 - train(rank0) - INFO -     epoch          : 34
2025-05-22 12:40:52,905 - train(rank0) - INFO -     loss           : 0.17029282078146935
2025-05-22 12:40:52,905 - train(rank0) - INFO -     loss_mbce      : 0.04501884281635284
2025-05-22 12:40:52,906 - train(rank0) - INFO -     loss_pkd       : 0.005894112735404633
2025-05-22 12:40:52,906 - train(rank0) - INFO -     loss_cont      : 0.05801567643880845
2025-05-22 12:40:52,906 - train(rank0) - INFO -     loss_uncer     : 0.06136418491601945
2025-05-22 12:40:52,914 - train(rank0) - INFO - Epoch - 35
2025-05-22 12:40:58,766 - train(rank0) - INFO - lr[0]: 0.000047 / lr[1]: 0.000471 / lr[2]: 0.000471
2025-05-22 12:40:58,766 - train(rank0) - INFO - [0/20]
2025-05-22 12:41:01,167 - train(rank0) - INFO - [4/20]
2025-05-22 12:41:03,514 - train(rank0) - INFO - [8/20]
2025-05-22 12:41:05,929 - train(rank0) - INFO - [12/20]
2025-05-22 12:41:08,295 - train(rank0) - INFO - [16/20]
2025-05-22 12:41:10,499 - train(rank0) - INFO -     epoch          : 35
2025-05-22 12:41:10,500 - train(rank0) - INFO -     loss           : 0.17499599307775499
2025-05-22 12:41:10,500 - train(rank0) - INFO -     loss_mbce      : 0.04334353758022189
2025-05-22 12:41:10,500 - train(rank0) - INFO -     loss_pkd       : 0.009718385619635228
2025-05-22 12:41:10,500 - train(rank0) - INFO -     loss_cont      : 0.058174833059310926
2025-05-22 12:41:10,501 - train(rank0) - INFO -     loss_uncer     : 0.06375923410058022
2025-05-22 12:41:10,518 - train(rank0) - INFO - Epoch - 36
2025-05-22 12:41:16,320 - train(rank0) - INFO - lr[0]: 0.000045 / lr[1]: 0.000455 / lr[2]: 0.000455
2025-05-22 12:41:16,321 - train(rank0) - INFO - [0/20]
2025-05-22 12:41:18,706 - train(rank0) - INFO - [4/20]
2025-05-22 12:41:21,053 - train(rank0) - INFO - [8/20]
2025-05-22 12:41:23,284 - train(rank0) - INFO - [12/20]
2025-05-22 12:41:25,615 - train(rank0) - INFO - [16/20]
2025-05-22 12:41:27,822 - train(rank0) - INFO -     epoch          : 36
2025-05-22 12:41:27,823 - train(rank0) - INFO -     loss           : 0.17492027655243875
2025-05-22 12:41:27,823 - train(rank0) - INFO -     loss_mbce      : 0.043688719440251586
2025-05-22 12:41:27,823 - train(rank0) - INFO -     loss_pkd       : 0.008848191493598279
2025-05-22 12:41:27,824 - train(rank0) - INFO -     loss_cont      : 0.057745963037014
2025-05-22 12:41:27,824 - train(rank0) - INFO -     loss_uncer     : 0.06463739901781082
2025-05-22 12:41:27,844 - train(rank0) - INFO - Epoch - 37
2025-05-22 12:41:33,775 - train(rank0) - INFO - lr[0]: 0.000044 / lr[1]: 0.000438 / lr[2]: 0.000438
2025-05-22 12:41:33,775 - train(rank0) - INFO - [0/20]
2025-05-22 12:41:36,112 - train(rank0) - INFO - [4/20]
2025-05-22 12:41:38,463 - train(rank0) - INFO - [8/20]
2025-05-22 12:41:40,806 - train(rank0) - INFO - [12/20]
2025-05-22 12:41:43,215 - train(rank0) - INFO - [16/20]
2025-05-22 12:41:45,411 - train(rank0) - INFO -     epoch          : 37
2025-05-22 12:41:45,412 - train(rank0) - INFO -     loss           : 0.17167531847953796
2025-05-22 12:41:45,412 - train(rank0) - INFO -     loss_mbce      : 0.04666724894195795
2025-05-22 12:41:45,412 - train(rank0) - INFO -     loss_pkd       : 0.005310034364811145
2025-05-22 12:41:45,413 - train(rank0) - INFO -     loss_cont      : 0.057938597202301025
2025-05-22 12:41:45,413 - train(rank0) - INFO -     loss_uncer     : 0.06175943851470948
2025-05-22 12:41:45,464 - train(rank0) - INFO - Epoch - 38
2025-05-22 12:41:51,200 - train(rank0) - INFO - lr[0]: 0.000042 / lr[1]: 0.000422 / lr[2]: 0.000422
2025-05-22 12:41:51,200 - train(rank0) - INFO - [0/20]
2025-05-22 12:41:53,601 - train(rank0) - INFO - [4/20]
2025-05-22 12:41:55,954 - train(rank0) - INFO - [8/20]
2025-05-22 12:41:58,316 - train(rank0) - INFO - [12/20]
2025-05-22 12:42:00,686 - train(rank0) - INFO - [16/20]
2025-05-22 12:42:02,998 - train(rank0) - INFO -     epoch          : 38
2025-05-22 12:42:02,999 - train(rank0) - INFO -     loss           : 0.1745329461991787
2025-05-22 12:42:02,999 - train(rank0) - INFO -     loss_mbce      : 0.05046333698555827
2025-05-22 12:42:02,999 - train(rank0) - INFO -     loss_pkd       : 0.005059636423538905
2025-05-22 12:42:03,000 - train(rank0) - INFO -     loss_cont      : 0.0574728187918663
2025-05-22 12:42:03,000 - train(rank0) - INFO -     loss_uncer     : 0.06153715118765831
2025-05-22 12:42:03,008 - train(rank0) - INFO - Epoch - 39
2025-05-22 12:42:08,956 - train(rank0) - INFO - lr[0]: 0.000041 / lr[1]: 0.000405 / lr[2]: 0.000405
2025-05-22 12:42:08,957 - train(rank0) - INFO - [0/20]
2025-05-22 12:42:11,363 - train(rank0) - INFO - [4/20]
2025-05-22 12:42:13,709 - train(rank0) - INFO - [8/20]
2025-05-22 12:42:16,095 - train(rank0) - INFO - [12/20]
2025-05-22 12:42:18,427 - train(rank0) - INFO - [16/20]
2025-05-22 12:42:20,654 - train(rank0) - INFO -     epoch          : 39
2025-05-22 12:42:20,655 - train(rank0) - INFO -     loss           : 0.1678982488811016
2025-05-22 12:42:20,655 - train(rank0) - INFO -     loss_mbce      : 0.04320965511724353
2025-05-22 12:42:20,655 - train(rank0) - INFO -     loss_pkd       : 0.004880919623246882
2025-05-22 12:42:20,655 - train(rank0) - INFO -     loss_cont      : 0.05753199338912964
2025-05-22 12:42:20,655 - train(rank0) - INFO -     loss_uncer     : 0.06227568000555039
2025-05-22 12:42:20,671 - train(rank0) - INFO - Epoch - 40
2025-05-22 12:42:26,390 - train(rank0) - INFO - lr[0]: 0.000039 / lr[1]: 0.000389 / lr[2]: 0.000389
2025-05-22 12:42:26,391 - train(rank0) - INFO - [0/20]
2025-05-22 12:42:28,807 - train(rank0) - INFO - [4/20]
2025-05-22 12:42:31,201 - train(rank0) - INFO - [8/20]
2025-05-22 12:42:33,593 - train(rank0) - INFO - [12/20]
2025-05-22 12:42:35,934 - train(rank0) - INFO - [16/20]
2025-05-22 12:42:38,076 - train(rank0) - INFO - Number of val loader: 84
2025-05-22 12:42:42,180 - train(rank0) - INFO -     epoch          : 40
2025-05-22 12:42:42,181 - train(rank0) - INFO -     loss           : 0.1841094933450222
2025-05-22 12:42:42,181 - train(rank0) - INFO -     loss_mbce      : 0.04769336497411132
2025-05-22 12:42:42,181 - train(rank0) - INFO -     loss_pkd       : 0.015087704188772477
2025-05-22 12:42:42,181 - train(rank0) - INFO -     loss_cont      : 0.05742351680994033
2025-05-22 12:42:42,181 - train(rank0) - INFO -     loss_uncer     : 0.06390490502119064
2025-05-22 12:42:42,181 - train(rank0) - INFO -     val_Pixel_Accuracy_old: 97.63
2025-05-22 12:42:42,181 - train(rank0) - INFO -     val_Pixel_Accuracy_new: 81.10
2025-05-22 12:42:42,182 - train(rank0) - INFO -     val_Pixel_Accuracy_harmonic: 88.60
2025-05-22 12:42:42,182 - train(rank0) - INFO -     val_Pixel_Accuracy_overall: 92.18
2025-05-22 12:42:42,182 - train(rank0) - INFO -     val_Pixel_Accuracy_Class_old: 97.63
2025-05-22 12:42:42,182 - train(rank0) - INFO -     val_Pixel_Accuracy_Class_new: 81.10
2025-05-22 12:42:42,182 - train(rank0) - INFO -     val_Pixel_Accuracy_Class_harmonic: 88.60
2025-05-22 12:42:42,182 - train(rank0) - INFO -     val_Pixel_Accuracy_Class_overall: 89.37
2025-05-22 12:42:42,182 - train(rank0) - INFO -     val_Pixel_Accuracy_Class_by_class: 
 0  background 97.63
19 *train 81.10

2025-05-22 12:42:42,182 - train(rank0) - INFO -     val_Mean_Intersection_over_Union_old: 90.20
2025-05-22 12:42:42,182 - train(rank0) - INFO -     val_Mean_Intersection_over_Union_new: 80.18
2025-05-22 12:42:42,182 - train(rank0) - INFO -     val_Mean_Intersection_over_Union_harmonic: 84.89
2025-05-22 12:42:42,182 - train(rank0) - INFO -     val_Mean_Intersection_over_Union_overall: 85.19
2025-05-22 12:42:42,182 - train(rank0) - INFO -     val_Mean_Intersection_over_Union_by_class: 
 0  background 90.20
19 *train 80.18

2025-05-22 12:42:42,873 - train(rank0) - INFO - Saving checkpoint: saved_voc/models/overlap_15-1_Adapter/step_4/checkpoint-epoch60.pth ...
2025-05-22 12:42:42,874 - train(rank0) - INFO - computing prototypes...
2025-05-22 12:42:47,446 - train(rank0) - INFO - [0/20]
2025-05-22 12:42:47,940 - train(rank0) - INFO - [4/20]
2025-05-22 12:42:48,430 - train(rank0) - INFO - [8/20]
2025-05-22 12:42:48,920 - train(rank0) - INFO - [12/20]
2025-05-22 12:42:49,414 - train(rank0) - INFO - [16/20]
2025-05-22 12:42:50,080 - train(rank0) - INFO - computing noise...
2025-05-22 12:42:54,848 - train(rank0) - INFO - [0/20]
2025-05-22 12:42:55,347 - train(rank0) - INFO - [4/20]
2025-05-22 12:42:55,844 - train(rank0) - INFO - [8/20]
2025-05-22 12:42:56,345 - train(rank0) - INFO - [12/20]
2025-05-22 12:42:56,842 - train(rank0) - INFO - [16/20]
2025-05-22 12:42:57,557 - train(rank0) - INFO - Epoch - 41
2025-05-22 12:43:03,170 - train(rank0) - INFO - lr[0]: 0.000037 / lr[1]: 0.000372 / lr[2]: 0.000372
2025-05-22 12:43:03,171 - train(rank0) - INFO - [0/20]
2025-05-22 12:43:05,525 - train(rank0) - INFO - [4/20]
2025-05-22 12:43:07,845 - train(rank0) - INFO - [8/20]
2025-05-22 12:43:10,060 - train(rank0) - INFO - [12/20]
2025-05-22 12:43:12,325 - train(rank0) - INFO - [16/20]
2025-05-22 12:43:14,500 - train(rank0) - INFO -     epoch          : 41
2025-05-22 12:43:14,500 - train(rank0) - INFO -     loss           : 0.17449234873056413
2025-05-22 12:43:14,501 - train(rank0) - INFO -     loss_mbce      : 0.042158821038901804
2025-05-22 12:43:14,501 - train(rank0) - INFO -     loss_pkd       : 0.006628977382206358
2025-05-22 12:43:14,501 - train(rank0) - INFO -     loss_cont      : 0.05913736522197723
2025-05-22 12:43:14,501 - train(rank0) - INFO -     loss_uncer     : 0.06656718164682388
2025-05-22 12:43:14,542 - train(rank0) - INFO - Epoch - 42
2025-05-22 12:43:20,426 - train(rank0) - INFO - lr[0]: 0.000036 / lr[1]: 0.000355 / lr[2]: 0.000355
2025-05-22 12:43:20,426 - train(rank0) - INFO - [0/20]
2025-05-22 12:43:22,761 - train(rank0) - INFO - [4/20]
2025-05-22 12:43:25,138 - train(rank0) - INFO - [8/20]
2025-05-22 12:43:27,457 - train(rank0) - INFO - [12/20]
2025-05-22 12:43:29,767 - train(rank0) - INFO - [16/20]
2025-05-22 12:43:31,976 - train(rank0) - INFO -     epoch          : 42
2025-05-22 12:43:31,977 - train(rank0) - INFO -     loss           : 0.16906642243266107
2025-05-22 12:43:31,978 - train(rank0) - INFO -     loss_mbce      : 0.044023124128580095
2025-05-22 12:43:31,978 - train(rank0) - INFO -     loss_pkd       : 0.005435280152596533
2025-05-22 12:43:31,978 - train(rank0) - INFO -     loss_cont      : 0.0574333381652832
2025-05-22 12:43:31,978 - train(rank0) - INFO -     loss_uncer     : 0.062174675762653356
2025-05-22 12:43:31,995 - train(rank0) - INFO - Epoch - 43
2025-05-22 12:43:38,129 - train(rank0) - INFO - lr[0]: 0.000034 / lr[1]: 0.000338 / lr[2]: 0.000338
2025-05-22 12:43:38,129 - train(rank0) - INFO - [0/20]
2025-05-22 12:43:40,501 - train(rank0) - INFO - [4/20]
2025-05-22 12:43:42,866 - train(rank0) - INFO - [8/20]
2025-05-22 12:43:45,237 - train(rank0) - INFO - [12/20]
2025-05-22 12:43:47,613 - train(rank0) - INFO - [16/20]
2025-05-22 12:43:49,767 - train(rank0) - INFO -     epoch          : 43
2025-05-22 12:43:49,768 - train(rank0) - INFO -     loss           : 0.16957710087299346
2025-05-22 12:43:49,768 - train(rank0) - INFO -     loss_mbce      : 0.04596699513494969
2025-05-22 12:43:49,769 - train(rank0) - INFO -     loss_pkd       : 0.005525263288291171
2025-05-22 12:43:49,769 - train(rank0) - INFO -     loss_cont      : 0.056773659288883206
2025-05-22 12:43:49,769 - train(rank0) - INFO -     loss_uncer     : 0.06131118252873422
2025-05-22 12:43:49,839 - train(rank0) - INFO - Epoch - 44
2025-05-22 12:43:55,703 - train(rank0) - INFO - lr[0]: 0.000032 / lr[1]: 0.000321 / lr[2]: 0.000321
2025-05-22 12:43:55,709 - train(rank0) - INFO - [0/20]
2025-05-22 12:43:58,011 - train(rank0) - INFO - [4/20]
2025-05-22 12:44:00,296 - train(rank0) - INFO - [8/20]
2025-05-22 12:44:02,676 - train(rank0) - INFO - [12/20]
2025-05-22 12:44:05,040 - train(rank0) - INFO - [16/20]
2025-05-22 12:44:07,285 - train(rank0) - INFO -     epoch          : 44
2025-05-22 12:44:07,286 - train(rank0) - INFO -     loss           : 0.1745233066380024
2025-05-22 12:44:07,287 - train(rank0) - INFO -     loss_mbce      : 0.04816554421558976
2025-05-22 12:44:07,287 - train(rank0) - INFO -     loss_pkd       : 0.004940228784107603
2025-05-22 12:44:07,287 - train(rank0) - INFO -     loss_cont      : 0.05767952948808671
2025-05-22 12:44:07,287 - train(rank0) - INFO -     loss_uncer     : 0.06373800218105316
2025-05-22 12:44:07,299 - train(rank0) - INFO - Epoch - 45
2025-05-22 12:44:13,030 - train(rank0) - INFO - lr[0]: 0.000030 / lr[1]: 0.000304 / lr[2]: 0.000304
2025-05-22 12:44:13,031 - train(rank0) - INFO - [0/20]
2025-05-22 12:44:15,386 - train(rank0) - INFO - [4/20]
2025-05-22 12:44:17,614 - train(rank0) - INFO - [8/20]
2025-05-22 12:44:19,948 - train(rank0) - INFO - [12/20]
2025-05-22 12:44:22,333 - train(rank0) - INFO - [16/20]
2025-05-22 12:44:24,476 - train(rank0) - INFO -     epoch          : 45
2025-05-22 12:44:24,477 - train(rank0) - INFO -     loss           : 0.17157667577266694
2025-05-22 12:44:24,477 - train(rank0) - INFO -     loss_mbce      : 0.047812992334365846
2025-05-22 12:44:24,477 - train(rank0) - INFO -     loss_pkd       : 0.0042755491158459336
2025-05-22 12:44:24,477 - train(rank0) - INFO -     loss_cont      : 0.05721842348575591
2025-05-22 12:44:24,478 - train(rank0) - INFO -     loss_uncer     : 0.062269709408283226
2025-05-22 12:44:24,523 - train(rank0) - INFO - Epoch - 46
2025-05-22 12:44:30,464 - train(rank0) - INFO - lr[0]: 0.000029 / lr[1]: 0.000287 / lr[2]: 0.000287
2025-05-22 12:44:30,464 - train(rank0) - INFO - [0/20]
2025-05-22 12:44:32,815 - train(rank0) - INFO - [4/20]
2025-05-22 12:44:35,202 - train(rank0) - INFO - [8/20]
2025-05-22 12:44:37,636 - train(rank0) - INFO - [12/20]
2025-05-22 12:44:39,958 - train(rank0) - INFO - [16/20]
2025-05-22 12:44:42,086 - train(rank0) - INFO -     epoch          : 46
2025-05-22 12:44:42,087 - train(rank0) - INFO -     loss           : 0.17758656963706015
2025-05-22 12:44:42,087 - train(rank0) - INFO -     loss_mbce      : 0.046422408893704414
2025-05-22 12:44:42,087 - train(rank0) - INFO -     loss_pkd       : 0.012934715210576542
2025-05-22 12:44:42,088 - train(rank0) - INFO -     loss_cont      : 0.05726027965545655
2025-05-22 12:44:42,088 - train(rank0) - INFO -     loss_uncer     : 0.0609691621363163
2025-05-22 12:44:42,097 - train(rank0) - INFO - Epoch - 47
2025-05-22 12:44:47,908 - train(rank0) - INFO - lr[0]: 0.000027 / lr[1]: 0.000270 / lr[2]: 0.000270
2025-05-22 12:44:47,908 - train(rank0) - INFO - [0/20]
2025-05-22 12:44:50,311 - train(rank0) - INFO - [4/20]
2025-05-22 12:44:52,661 - train(rank0) - INFO - [8/20]
2025-05-22 12:44:55,018 - train(rank0) - INFO - [12/20]
2025-05-22 12:44:57,259 - train(rank0) - INFO - [16/20]
2025-05-22 12:44:59,402 - train(rank0) - INFO -     epoch          : 47
2025-05-22 12:44:59,403 - train(rank0) - INFO -     loss           : 0.16809658333659172
2025-05-22 12:44:59,403 - train(rank0) - INFO -     loss_mbce      : 0.040011193789541724
2025-05-22 12:44:59,403 - train(rank0) - INFO -     loss_pkd       : 0.006121882746811025
2025-05-22 12:44:59,403 - train(rank0) - INFO -     loss_cont      : 0.057449919283390063
2025-05-22 12:44:59,403 - train(rank0) - INFO -     loss_uncer     : 0.06451358661055566
2025-05-22 12:44:59,426 - train(rank0) - INFO - Epoch - 48
2025-05-22 12:45:05,148 - train(rank0) - INFO - lr[0]: 0.000025 / lr[1]: 0.000252 / lr[2]: 0.000252
2025-05-22 12:45:05,148 - train(rank0) - INFO - [0/20]
2025-05-22 12:45:07,555 - train(rank0) - INFO - [4/20]
2025-05-22 12:45:09,917 - train(rank0) - INFO - [8/20]
2025-05-22 12:45:12,245 - train(rank0) - INFO - [12/20]
2025-05-22 12:45:14,444 - train(rank0) - INFO - [16/20]
2025-05-22 12:45:16,574 - train(rank0) - INFO -     epoch          : 48
2025-05-22 12:45:16,575 - train(rank0) - INFO -     loss           : 0.16890789642930032
2025-05-22 12:45:16,575 - train(rank0) - INFO -     loss_mbce      : 0.04225988388061523
2025-05-22 12:45:16,575 - train(rank0) - INFO -     loss_pkd       : 0.0060466164504759945
2025-05-22 12:45:16,575 - train(rank0) - INFO -     loss_cont      : 0.056681337356567385
2025-05-22 12:45:16,576 - train(rank0) - INFO -     loss_uncer     : 0.06392005622386933
2025-05-22 12:45:16,661 - train(rank0) - INFO - Epoch - 49
2025-05-22 12:45:22,478 - train(rank0) - INFO - lr[0]: 0.000023 / lr[1]: 0.000235 / lr[2]: 0.000235
2025-05-22 12:45:22,478 - train(rank0) - INFO - [0/20]
2025-05-22 12:45:24,861 - train(rank0) - INFO - [4/20]
2025-05-22 12:45:27,206 - train(rank0) - INFO - [8/20]
2025-05-22 12:45:29,521 - train(rank0) - INFO - [12/20]
2025-05-22 12:45:31,884 - train(rank0) - INFO - [16/20]
2025-05-22 12:45:34,109 - train(rank0) - INFO -     epoch          : 49
2025-05-22 12:45:34,109 - train(rank0) - INFO -     loss           : 0.17081729769706727
2025-05-22 12:45:34,110 - train(rank0) - INFO -     loss_mbce      : 0.04675048450008035
2025-05-22 12:45:34,110 - train(rank0) - INFO -     loss_pkd       : 0.005374106716772076
2025-05-22 12:45:34,110 - train(rank0) - INFO -     loss_cont      : 0.05698609322309493
2025-05-22 12:45:34,110 - train(rank0) - INFO -     loss_uncer     : 0.061706613898277286
2025-05-22 12:45:34,121 - train(rank0) - INFO - Epoch - 50
2025-05-22 12:45:39,996 - train(rank0) - INFO - lr[0]: 0.000022 / lr[1]: 0.000217 / lr[2]: 0.000217
2025-05-22 12:45:39,997 - train(rank0) - INFO - [0/20]
2025-05-22 12:45:42,368 - train(rank0) - INFO - [4/20]
2025-05-22 12:45:44,744 - train(rank0) - INFO - [8/20]
2025-05-22 12:45:47,077 - train(rank0) - INFO - [12/20]
2025-05-22 12:45:49,286 - train(rank0) - INFO - [16/20]
2025-05-22 12:45:51,432 - train(rank0) - INFO - Number of val loader: 84
2025-05-22 12:45:55,645 - train(rank0) - INFO -     epoch          : 50
2025-05-22 12:45:55,645 - train(rank0) - INFO -     loss           : 0.17030410915613176
2025-05-22 12:45:55,646 - train(rank0) - INFO -     loss_mbce      : 0.045996835082769395
2025-05-22 12:45:55,646 - train(rank0) - INFO -     loss_pkd       : 0.005802353531180415
2025-05-22 12:45:55,646 - train(rank0) - INFO -     loss_cont      : 0.0572232234477997
2025-05-22 12:45:55,646 - train(rank0) - INFO -     loss_uncer     : 0.06128169238567353
2025-05-22 12:45:55,646 - train(rank0) - INFO -     val_Pixel_Accuracy_old: 97.65
2025-05-22 12:45:55,646 - train(rank0) - INFO -     val_Pixel_Accuracy_new: 81.35
2025-05-22 12:45:55,646 - train(rank0) - INFO -     val_Pixel_Accuracy_harmonic: 88.76
2025-05-22 12:45:55,646 - train(rank0) - INFO -     val_Pixel_Accuracy_overall: 92.27
2025-05-22 12:45:55,646 - train(rank0) - INFO -     val_Pixel_Accuracy_Class_old: 97.65
2025-05-22 12:45:55,646 - train(rank0) - INFO -     val_Pixel_Accuracy_Class_new: 81.35
2025-05-22 12:45:55,646 - train(rank0) - INFO -     val_Pixel_Accuracy_Class_harmonic: 88.76
2025-05-22 12:45:55,646 - train(rank0) - INFO -     val_Pixel_Accuracy_Class_overall: 89.50
2025-05-22 12:45:55,647 - train(rank0) - INFO -     val_Pixel_Accuracy_Class_by_class: 
 0  background 97.65
19 *train 81.35

2025-05-22 12:45:55,647 - train(rank0) - INFO -     val_Mean_Intersection_over_Union_old: 90.28
2025-05-22 12:45:55,647 - train(rank0) - INFO -     val_Mean_Intersection_over_Union_new: 80.44
2025-05-22 12:45:55,647 - train(rank0) - INFO -     val_Mean_Intersection_over_Union_harmonic: 85.07
2025-05-22 12:45:55,647 - train(rank0) - INFO -     val_Mean_Intersection_over_Union_overall: 85.36
2025-05-22 12:45:55,647 - train(rank0) - INFO -     val_Mean_Intersection_over_Union_by_class: 
 0  background 90.28
19 *train 80.44

2025-05-22 12:45:56,371 - train(rank0) - INFO - Saving checkpoint: saved_voc/models/overlap_15-1_Adapter/step_4/checkpoint-epoch60.pth ...
2025-05-22 12:45:56,372 - train(rank0) - INFO - computing prototypes...
2025-05-22 12:46:01,271 - train(rank0) - INFO - [0/20]
2025-05-22 12:46:01,772 - train(rank0) - INFO - [4/20]
2025-05-22 12:46:02,262 - train(rank0) - INFO - [8/20]
2025-05-22 12:46:02,750 - train(rank0) - INFO - [12/20]
2025-05-22 12:46:03,239 - train(rank0) - INFO - [16/20]
2025-05-22 12:46:03,895 - train(rank0) - INFO - computing noise...
2025-05-22 12:46:08,908 - train(rank0) - INFO - [0/20]
2025-05-22 12:46:09,416 - train(rank0) - INFO - [4/20]
2025-05-22 12:46:09,917 - train(rank0) - INFO - [8/20]
2025-05-22 12:46:10,415 - train(rank0) - INFO - [12/20]
2025-05-22 12:46:10,913 - train(rank0) - INFO - [16/20]
2025-05-22 12:46:11,649 - train(rank0) - INFO - Epoch - 51
2025-05-22 12:46:17,417 - train(rank0) - INFO - lr[0]: 0.000020 / lr[1]: 0.000199 / lr[2]: 0.000199
2025-05-22 12:46:17,418 - train(rank0) - INFO - [0/20]
2025-05-22 12:46:19,794 - train(rank0) - INFO - [4/20]
2025-05-22 12:46:22,190 - train(rank0) - INFO - [8/20]
2025-05-22 12:46:24,352 - train(rank0) - INFO - [12/20]
2025-05-22 12:46:26,615 - train(rank0) - INFO - [16/20]
2025-05-22 12:46:28,745 - train(rank0) - INFO -     epoch          : 51
2025-05-22 12:46:28,746 - train(rank0) - INFO -     loss           : 0.17070661187171937
2025-05-22 12:46:28,746 - train(rank0) - INFO -     loss_mbce      : 0.04584882473573089
2025-05-22 12:46:28,746 - train(rank0) - INFO -     loss_pkd       : 0.005709950099117123
2025-05-22 12:46:28,747 - train(rank0) - INFO -     loss_cont      : 0.05653606772422791
2025-05-22 12:46:28,747 - train(rank0) - INFO -     loss_uncer     : 0.06261176601052285
2025-05-22 12:46:28,774 - train(rank0) - INFO - Epoch - 52
2025-05-22 12:46:34,570 - train(rank0) - INFO - lr[0]: 0.000018 / lr[1]: 0.000181 / lr[2]: 0.000181
2025-05-22 12:46:34,570 - train(rank0) - INFO - [0/20]
2025-05-22 12:46:36,881 - train(rank0) - INFO - [4/20]
2025-05-22 12:46:39,237 - train(rank0) - INFO - [8/20]
2025-05-22 12:46:41,594 - train(rank0) - INFO - [12/20]
2025-05-22 12:46:43,972 - train(rank0) - INFO - [16/20]
2025-05-22 12:46:46,108 - train(rank0) - INFO -     epoch          : 52
2025-05-22 12:46:46,109 - train(rank0) - INFO -     loss           : 0.16792559400200843
2025-05-22 12:46:46,109 - train(rank0) - INFO -     loss_mbce      : 0.04227630952373147
2025-05-22 12:46:46,109 - train(rank0) - INFO -     loss_pkd       : 0.003936091423383914
2025-05-22 12:46:46,110 - train(rank0) - INFO -     loss_cont      : 0.05682377785444259
2025-05-22 12:46:46,110 - train(rank0) - INFO -     loss_uncer     : 0.06488940939307213
2025-05-22 12:46:46,144 - train(rank0) - INFO - Epoch - 53
2025-05-22 12:46:52,102 - train(rank0) - INFO - lr[0]: 0.000016 / lr[1]: 0.000163 / lr[2]: 0.000163
2025-05-22 12:46:52,102 - train(rank0) - INFO - [0/20]
2025-05-22 12:46:54,450 - train(rank0) - INFO - [4/20]
2025-05-22 12:46:56,786 - train(rank0) - INFO - [8/20]
2025-05-22 12:46:59,095 - train(rank0) - INFO - [12/20]
2025-05-22 12:47:01,458 - train(rank0) - INFO - [16/20]
2025-05-22 12:47:03,604 - train(rank0) - INFO -     epoch          : 53
2025-05-22 12:47:03,605 - train(rank0) - INFO -     loss           : 0.16997012197971345
2025-05-22 12:47:03,605 - train(rank0) - INFO -     loss_mbce      : 0.04469299325719476
2025-05-22 12:47:03,605 - train(rank0) - INFO -     loss_pkd       : 0.006077449485019315
2025-05-22 12:47:03,606 - train(rank0) - INFO -     loss_cont      : 0.05645768493413925
2025-05-22 12:47:03,606 - train(rank0) - INFO -     loss_uncer     : 0.06274199098348616
2025-05-22 12:47:03,684 - train(rank0) - INFO - Epoch - 54
2025-05-22 12:47:09,531 - train(rank0) - INFO - lr[0]: 0.000014 / lr[1]: 0.000145 / lr[2]: 0.000145
2025-05-22 12:47:09,532 - train(rank0) - INFO - [0/20]
2025-05-22 12:47:11,898 - train(rank0) - INFO - [4/20]
2025-05-22 12:47:14,252 - train(rank0) - INFO - [8/20]
2025-05-22 12:47:16,655 - train(rank0) - INFO - [12/20]
2025-05-22 12:47:19,055 - train(rank0) - INFO - [16/20]
2025-05-22 12:47:21,164 - train(rank0) - INFO -     epoch          : 54
2025-05-22 12:47:21,165 - train(rank0) - INFO -     loss           : 0.17526369616389276
2025-05-22 12:47:21,165 - train(rank0) - INFO -     loss_mbce      : 0.05080856774002314
2025-05-22 12:47:21,165 - train(rank0) - INFO -     loss_pkd       : 0.005312207562383264
2025-05-22 12:47:21,166 - train(rank0) - INFO -     loss_cont      : 0.056955789327621464
2025-05-22 12:47:21,166 - train(rank0) - INFO -     loss_uncer     : 0.06218712940812112
2025-05-22 12:47:21,235 - train(rank0) - INFO - Epoch - 55
2025-05-22 12:47:27,099 - train(rank0) - INFO - lr[0]: 0.000013 / lr[1]: 0.000126 / lr[2]: 0.000126
2025-05-22 12:47:27,099 - train(rank0) - INFO - [0/20]
2025-05-22 12:47:29,430 - train(rank0) - INFO - [4/20]
2025-05-22 12:47:31,829 - train(rank0) - INFO - [8/20]
2025-05-22 12:47:34,159 - train(rank0) - INFO - [12/20]
2025-05-22 12:47:36,511 - train(rank0) - INFO - [16/20]
2025-05-22 12:47:38,703 - train(rank0) - INFO -     epoch          : 55
2025-05-22 12:47:38,704 - train(rank0) - INFO -     loss           : 0.17345334067940713
2025-05-22 12:47:38,704 - train(rank0) - INFO -     loss_mbce      : 0.047745689563453195
2025-05-22 12:47:38,704 - train(rank0) - INFO -     loss_pkd       : 0.00683820599806495
2025-05-22 12:47:38,704 - train(rank0) - INFO -     loss_cont      : 0.05777252644300461
2025-05-22 12:47:38,704 - train(rank0) - INFO -     loss_uncer     : 0.06109691619873048
2025-05-22 12:47:38,730 - train(rank0) - INFO - Epoch - 56
2025-05-22 12:47:44,525 - train(rank0) - INFO - lr[0]: 0.000011 / lr[1]: 0.000107 / lr[2]: 0.000107
2025-05-22 12:47:44,525 - train(rank0) - INFO - [0/20]
2025-05-22 12:47:46,766 - train(rank0) - INFO - [4/20]
2025-05-22 12:47:49,128 - train(rank0) - INFO - [8/20]
2025-05-22 12:47:51,459 - train(rank0) - INFO - [12/20]
2025-05-22 12:47:53,851 - train(rank0) - INFO - [16/20]
2025-05-22 12:47:56,020 - train(rank0) - INFO -     epoch          : 56
2025-05-22 12:47:56,021 - train(rank0) - INFO -     loss           : 0.17703374326229096
2025-05-22 12:47:56,021 - train(rank0) - INFO -     loss_mbce      : 0.05256841704249382
2025-05-22 12:47:56,022 - train(rank0) - INFO -     loss_pkd       : 0.005752115030190907
2025-05-22 12:47:56,022 - train(rank0) - INFO -     loss_cont      : 0.057220441401004796
2025-05-22 12:47:56,022 - train(rank0) - INFO -     loss_uncer     : 0.061492769867181774
2025-05-22 12:47:56,032 - train(rank0) - INFO - Epoch - 57
2025-05-22 12:48:01,685 - train(rank0) - INFO - lr[0]: 0.000009 / lr[1]: 0.000087 / lr[2]: 0.000087
2025-05-22 12:48:01,685 - train(rank0) - INFO - [0/20]
2025-05-22 12:48:04,032 - train(rank0) - INFO - [4/20]
2025-05-22 12:48:06,330 - train(rank0) - INFO - [8/20]
2025-05-22 12:48:08,645 - train(rank0) - INFO - [12/20]
2025-05-22 12:48:11,036 - train(rank0) - INFO - [16/20]
2025-05-22 12:48:13,276 - train(rank0) - INFO -     epoch          : 57
2025-05-22 12:48:13,277 - train(rank0) - INFO -     loss           : 0.1697921298444271
2025-05-22 12:48:13,278 - train(rank0) - INFO -     loss_mbce      : 0.04633990442380309
2025-05-22 12:48:13,278 - train(rank0) - INFO -     loss_pkd       : 0.005250660171441268
2025-05-22 12:48:13,278 - train(rank0) - INFO -     loss_cont      : 0.05735698521137238
2025-05-22 12:48:13,278 - train(rank0) - INFO -     loss_uncer     : 0.06084457725286484
2025-05-22 12:48:13,287 - train(rank0) - INFO - Epoch - 58
2025-05-22 12:48:19,250 - train(rank0) - INFO - lr[0]: 0.000007 / lr[1]: 0.000067 / lr[2]: 0.000067
2025-05-22 12:48:19,250 - train(rank0) - INFO - [0/20]
2025-05-22 12:48:21,422 - train(rank0) - INFO - [4/20]
2025-05-22 12:48:23,763 - train(rank0) - INFO - [8/20]
2025-05-22 12:48:25,947 - train(rank0) - INFO - [12/20]
2025-05-22 12:48:28,294 - train(rank0) - INFO - [16/20]
2025-05-22 12:48:30,480 - train(rank0) - INFO -     epoch          : 58
2025-05-22 12:48:30,480 - train(rank0) - INFO -     loss           : 0.17499866783618928
2025-05-22 12:48:30,481 - train(rank0) - INFO -     loss_mbce      : 0.04534718189388513
2025-05-22 12:48:30,481 - train(rank0) - INFO -     loss_pkd       : 0.008601746558269951
2025-05-22 12:48:30,481 - train(rank0) - INFO -     loss_cont      : 0.05788938850164412
2025-05-22 12:48:30,481 - train(rank0) - INFO -     loss_uncer     : 0.06316034853458405
2025-05-22 12:48:30,513 - train(rank0) - INFO - Epoch - 59
2025-05-22 12:48:36,116 - train(rank0) - INFO - lr[0]: 0.000005 / lr[1]: 0.000047 / lr[2]: 0.000047
2025-05-22 12:48:36,116 - train(rank0) - INFO - [0/20]
2025-05-22 12:48:38,505 - train(rank0) - INFO - [4/20]
2025-05-22 12:48:40,905 - train(rank0) - INFO - [8/20]
2025-05-22 12:48:43,115 - train(rank0) - INFO - [12/20]
2025-05-22 12:48:45,438 - train(rank0) - INFO - [16/20]
2025-05-22 12:48:47,573 - train(rank0) - INFO -     epoch          : 59
2025-05-22 12:48:47,574 - train(rank0) - INFO -     loss           : 0.16665877029299736
2025-05-22 12:48:47,574 - train(rank0) - INFO -     loss_mbce      : 0.042624097783118486
2025-05-22 12:48:47,574 - train(rank0) - INFO -     loss_pkd       : 0.004604570123774465
2025-05-22 12:48:47,575 - train(rank0) - INFO -     loss_cont      : 0.056906112134456656
2025-05-22 12:48:47,575 - train(rank0) - INFO -     loss_uncer     : 0.06252398818731307
2025-05-22 12:48:47,584 - train(rank0) - INFO - Epoch - 60
2025-05-22 12:48:53,434 - train(rank0) - INFO - lr[0]: 0.000003 / lr[1]: 0.000025 / lr[2]: 0.000025
2025-05-22 12:48:53,434 - train(rank0) - INFO - [0/20]
2025-05-22 12:48:55,865 - train(rank0) - INFO - [4/20]
2025-05-22 12:48:58,220 - train(rank0) - INFO - [8/20]
2025-05-22 12:49:00,559 - train(rank0) - INFO - [12/20]
2025-05-22 12:49:02,880 - train(rank0) - INFO - [16/20]
2025-05-22 12:49:05,135 - train(rank0) - INFO - Number of val loader: 84
2025-05-22 12:49:09,196 - train(rank0) - INFO -     epoch          : 60
2025-05-22 12:49:09,196 - train(rank0) - INFO -     loss           : 0.16597698554396628
2025-05-22 12:49:09,196 - train(rank0) - INFO -     loss_mbce      : 0.03993992665782571
2025-05-22 12:49:09,196 - train(rank0) - INFO -     loss_pkd       : 0.004907039903628174
2025-05-22 12:49:09,196 - train(rank0) - INFO -     loss_cont      : 0.05738296508789061
2025-05-22 12:49:09,197 - train(rank0) - INFO -     loss_uncer     : 0.06374705314636231
2025-05-22 12:49:09,197 - train(rank0) - INFO -     val_Pixel_Accuracy_old: 97.66
2025-05-22 12:49:09,197 - train(rank0) - INFO -     val_Pixel_Accuracy_new: 81.24
2025-05-22 12:49:09,197 - train(rank0) - INFO -     val_Pixel_Accuracy_harmonic: 88.69
2025-05-22 12:49:09,197 - train(rank0) - INFO -     val_Pixel_Accuracy_overall: 92.24
2025-05-22 12:49:09,197 - train(rank0) - INFO -     val_Pixel_Accuracy_Class_old: 97.66
2025-05-22 12:49:09,197 - train(rank0) - INFO -     val_Pixel_Accuracy_Class_new: 81.24
2025-05-22 12:49:09,197 - train(rank0) - INFO -     val_Pixel_Accuracy_Class_harmonic: 88.69
2025-05-22 12:49:09,197 - train(rank0) - INFO -     val_Pixel_Accuracy_Class_overall: 89.45
2025-05-22 12:49:09,197 - train(rank0) - INFO -     val_Pixel_Accuracy_Class_by_class: 
 0  background 97.66
19 *train 81.24

2025-05-22 12:49:09,197 - train(rank0) - INFO -     val_Mean_Intersection_over_Union_old: 90.24
2025-05-22 12:49:09,197 - train(rank0) - INFO -     val_Mean_Intersection_over_Union_new: 80.33
2025-05-22 12:49:09,198 - train(rank0) - INFO -     val_Mean_Intersection_over_Union_harmonic: 85.00
2025-05-22 12:49:09,198 - train(rank0) - INFO -     val_Mean_Intersection_over_Union_overall: 85.29
2025-05-22 12:49:09,198 - train(rank0) - INFO -     val_Mean_Intersection_over_Union_by_class: 
 0  background 90.24
19 *train 80.33

2025-05-22 12:49:09,198 - train(rank0) - INFO - Validation performance didn't improve for 60 epochs. Training stops.
2025-05-22 12:49:09,247 - train(rank0) - INFO - Number of test loader: 1421
2025-05-22 12:49:13,992 - train(rank0) - INFO - [0/1421]
2025-05-22 12:49:21,864 - train(rank0) - INFO - [284/1421]
2025-05-22 12:49:29,688 - train(rank0) - INFO - [568/1421]
2025-05-22 12:49:37,710 - train(rank0) - INFO - [852/1421]
2025-05-22 12:49:45,549 - train(rank0) - INFO - [1136/1421]
2025-05-22 12:49:53,477 - train(rank0) - INFO - [1420/1421]
2025-05-22 12:49:53,820 - train(rank0) - INFO -     Pixel_Accuracy_old: 93.83
2025-05-22 12:49:53,820 - train(rank0) - INFO -     Pixel_Accuracy_new: 69.07
2025-05-22 12:49:53,820 - train(rank0) - INFO -     Pixel_Accuracy_harmonic: 79.57
2025-05-22 12:49:53,821 - train(rank0) - INFO -     Pixel_Accuracy_overall: 92.58
2025-05-22 12:49:53,821 - train(rank0) - INFO -     Pixel_Accuracy_Class_old: 89.77
2025-05-22 12:49:53,821 - train(rank0) - INFO -     Pixel_Accuracy_Class_new: 63.69
2025-05-22 12:49:53,821 - train(rank0) - INFO -     Pixel_Accuracy_Class_harmonic: 74.51
2025-05-22 12:49:53,821 - train(rank0) - INFO -     Pixel_Accuracy_Class_overall: 84.55
2025-05-22 12:49:53,821 - train(rank0) - INFO -     Pixel_Accuracy_Class_by_class: 
 0  background 94.57
 1  aeroplane 96.73
 2  bicycle 92.63
 3  bird 94.60
 4  boat 91.00
 5  bottle 92.25
 6  bus 95.30
 7  car 94.55
 8  cat 97.81
 9  chair 50.47
10  cow 89.32
11  diningtable 66.61
12  dog 96.74
13  horse 95.27
14  motorbike 94.29
15  person 94.12
16 *pottedplant 39.36
17 *sheep 69.37
18 *sofa 64.78
19 *train 81.24

2025-05-22 12:49:53,821 - train(rank0) - INFO -     Mean_Intersection_over_Union_old: 79.97
2025-05-22 12:49:53,821 - train(rank0) - INFO -     Mean_Intersection_over_Union_new: 49.78
2025-05-22 12:49:53,821 - train(rank0) - INFO -     Mean_Intersection_over_Union_harmonic: 61.36
2025-05-22 12:49:53,821 - train(rank0) - INFO -     Mean_Intersection_over_Union_overall: 73.93
2025-05-22 12:49:53,821 - train(rank0) - INFO -     Mean_Intersection_over_Union_by_class: 
 0  background 91.19
 1  aeroplane 91.28
 2  bicycle 41.17
 3  bird 89.36
 4  boat 74.66
 5  bottle 82.38
 6  bus 92.02
 7  car 90.52
 8  cat 93.27
 9  chair 39.24
10  cow 82.26
11  diningtable 63.55
12  dog 88.33
13  horse 87.72
14  motorbike 86.58
15  person 86.06
16 *pottedplant 36.67
17 *sheep 63.50
18 *sofa 31.06
19 *train 67.89

