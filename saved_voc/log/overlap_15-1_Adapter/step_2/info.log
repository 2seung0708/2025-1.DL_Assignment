2025-05-22 11:56:59,604 - train(rank0) - INFO - overlap / 15-1 / step: 2
2025-05-22 11:56:59,605 - train(rank0) - INFO - The number of datasets: 299 / 57 / 1329
2025-05-22 11:56:59,605 - train(rank0) - INFO - Old Classes: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]
2025-05-22 11:56:59,605 - train(rank0) - INFO - New Classes: [17]
2025-05-22 11:57:00,526 - train(rank0) - INFO - DeepLabV3(
  (backbone): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): SyncBatchNorm(512, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): SyncBatchNorm(1024, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (6): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (7): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (8): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (9): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (10): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (11): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (12): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (13): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (14): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (15): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (16): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (17): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (18): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (19): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (20): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (21): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (22): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(2048, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(2048, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
        (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(2048, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
        (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(2048, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
  )
  (aspp): ASPP(
    (convs): ModuleList(
      (0): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): ASPPConv(
        (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6), bias=False)
        (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): ASPPConv(
        (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12), bias=False)
        (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (3): ASPPConv(
        (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(18, 18), dilation=(18, 18), bias=False)
        (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (4): ASPPPooling(
        (0): AdaptiveAvgPool2d(output_size=1)
        (1): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU(inplace=True)
      )
    )
    (project): Sequential(
      (0): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Dropout(p=0.1, inplace=False)
    )
    (last_conv): Sequential(
      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
  )
  (cls): ModuleList(
    (0): Conv2d(256, 15, kernel_size=(1, 1), stride=(1, 1))
    (1): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))
    (2): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))
  )
)
2025-05-22 11:57:00,893 - train(rank0) - INFO - Load weights from a previous step:saved_voc/models/overlap_15-1_Adapter/step_1/checkpoint-epoch60.pth
2025-05-22 11:57:01,344 - train(rank0) - INFO - ** Random Initialization **
2025-05-22 11:57:03,975 - train(rank0) - INFO - pos_weight - 4
2025-05-22 11:57:03,975 - train(rank0) - INFO - Total loss = 1 * L_mbce + 5 * L_pkd
2025-05-22 11:57:03,975 - train(rank0) - INFO - computing number of pixels...
2025-05-22 11:57:06,038 - train(rank0) - INFO - [0/12]
2025-05-22 11:57:06,040 - train(rank0) - INFO - [2/12]
2025-05-22 11:57:06,413 - train(rank0) - INFO - [4/12]
2025-05-22 11:57:06,415 - train(rank0) - INFO - [6/12]
2025-05-22 11:57:06,837 - train(rank0) - INFO - [8/12]
2025-05-22 11:57:06,840 - train(rank0) - INFO - [10/12]
2025-05-22 11:57:07,543 - train(rank0) - INFO - tensor([[43]])
2025-05-22 11:57:13,480 - train(rank1) - INFO - tensor([[43]])
2025-05-22 11:57:13,652 - train(rank2) - INFO - tensor([[43]])
2025-05-22 11:57:13,661 - train(rank0) - INFO - Epoch - 1
2025-05-22 11:57:13,661 - train(rank0) - INFO - computing pred number of pixels...
2025-05-22 11:57:16,698 - train(rank0) - INFO - [0/12]
2025-05-22 11:57:16,843 - train(rank0) - INFO - [2/12]
2025-05-22 11:57:17,076 - train(rank0) - INFO - [4/12]
2025-05-22 11:57:17,315 - train(rank0) - INFO - [6/12]
2025-05-22 11:57:17,554 - train(rank0) - INFO - [8/12]
2025-05-22 11:57:17,793 - train(rank0) - INFO - [10/12]
2025-05-22 11:57:23,767 - train(rank0) - INFO - lr[0]: 0.000100 / lr[1]: 0.001000 / lr[2]: 0.001000
2025-05-22 11:57:23,767 - train(rank0) - INFO - [0/12]
2025-05-22 11:57:23,837 - torch.nn.parallel.distributed - INFO - Reducer buckets have been rebuilt in this iteration.
2025-05-22 11:57:23,844 - torch.nn.parallel.distributed - INFO - Reducer buckets have been rebuilt in this iteration.
2025-05-22 11:57:23,844 - torch.nn.parallel.distributed - INFO - Reducer buckets have been rebuilt in this iteration.
2025-05-22 11:57:24,966 - train(rank0) - INFO - [2/12]
2025-05-22 11:57:26,141 - train(rank0) - INFO - [4/12]
2025-05-22 11:57:27,312 - train(rank0) - INFO - [6/12]
2025-05-22 11:57:28,506 - train(rank0) - INFO - [8/12]
2025-05-22 11:57:29,710 - train(rank0) - INFO - [10/12]
2025-05-22 11:57:30,744 - train(rank0) - INFO -     epoch          : 1
2025-05-22 11:57:30,745 - train(rank0) - INFO -     loss           : 0.33339598774909973
2025-05-22 11:57:30,745 - train(rank0) - INFO -     loss_mbce      : 0.1833903001000484
2025-05-22 11:57:30,745 - train(rank0) - INFO -     loss_pkd       : 0.006415399196460688
2025-05-22 11:57:30,745 - train(rank0) - INFO -     loss_cont      : 0.07571395138899485
2025-05-22 11:57:30,746 - train(rank0) - INFO -     loss_uncer     : 0.06787633672356605
2025-05-22 11:57:30,766 - train(rank0) - INFO - Epoch - 2
2025-05-22 11:57:33,469 - train(rank0) - INFO - lr[0]: 0.000098 / lr[1]: 0.000985 / lr[2]: 0.000985
2025-05-22 11:57:33,469 - train(rank0) - INFO - [0/12]
2025-05-22 11:57:34,669 - train(rank0) - INFO - [2/12]
2025-05-22 11:57:35,853 - train(rank0) - INFO - [4/12]
2025-05-22 11:57:37,020 - train(rank0) - INFO - [6/12]
2025-05-22 11:57:38,213 - train(rank0) - INFO - [8/12]
2025-05-22 11:57:39,382 - train(rank0) - INFO - [10/12]
2025-05-22 11:57:40,409 - train(rank0) - INFO -     epoch          : 2
2025-05-22 11:57:40,410 - train(rank0) - INFO -     loss           : 0.25090138614177704
2025-05-22 11:57:40,410 - train(rank0) - INFO -     loss_mbce      : 0.09609745598087709
2025-05-22 11:57:40,411 - train(rank0) - INFO -     loss_pkd       : 0.008231487178515332
2025-05-22 11:57:40,411 - train(rank0) - INFO -     loss_cont      : 0.07162621269623438
2025-05-22 11:57:40,411 - train(rank0) - INFO -     loss_uncer     : 0.07494622717301051
2025-05-22 11:57:40,420 - train(rank0) - INFO - Epoch - 3
2025-05-22 11:57:43,016 - train(rank0) - INFO - lr[0]: 0.000097 / lr[1]: 0.000970 / lr[2]: 0.000970
2025-05-22 11:57:43,016 - train(rank0) - INFO - [0/12]
2025-05-22 11:57:44,182 - train(rank0) - INFO - [2/12]
2025-05-22 11:57:45,384 - train(rank0) - INFO - [4/12]
2025-05-22 11:57:46,577 - train(rank0) - INFO - [6/12]
2025-05-22 11:57:47,758 - train(rank0) - INFO - [8/12]
2025-05-22 11:57:48,980 - train(rank0) - INFO - [10/12]
2025-05-22 11:57:50,042 - train(rank0) - INFO -     epoch          : 3
2025-05-22 11:57:50,043 - train(rank0) - INFO -     loss           : 0.2073094348112742
2025-05-22 11:57:50,043 - train(rank0) - INFO -     loss_mbce      : 0.06336219888180494
2025-05-22 11:57:50,043 - train(rank0) - INFO -     loss_pkd       : 0.008541758943465538
2025-05-22 11:57:50,044 - train(rank0) - INFO -     loss_cont      : 0.0651179850101471
2025-05-22 11:57:50,044 - train(rank0) - INFO -     loss_uncer     : 0.07028748989105223
2025-05-22 11:57:50,053 - train(rank0) - INFO - Epoch - 4
2025-05-22 11:57:52,650 - train(rank0) - INFO - lr[0]: 0.000095 / lr[1]: 0.000955 / lr[2]: 0.000955
2025-05-22 11:57:52,651 - train(rank0) - INFO - [0/12]
2025-05-22 11:57:53,858 - train(rank0) - INFO - [2/12]
2025-05-22 11:57:55,072 - train(rank0) - INFO - [4/12]
2025-05-22 11:57:56,274 - train(rank0) - INFO - [6/12]
2025-05-22 11:57:57,459 - train(rank0) - INFO - [8/12]
2025-05-22 11:57:58,620 - train(rank0) - INFO - [10/12]
2025-05-22 11:57:59,642 - train(rank0) - INFO -     epoch          : 4
2025-05-22 11:57:59,642 - train(rank0) - INFO -     loss           : 0.19781285772720972
2025-05-22 11:57:59,643 - train(rank0) - INFO -     loss_mbce      : 0.045384157759447895
2025-05-22 11:57:59,643 - train(rank0) - INFO -     loss_pkd       : 0.01003515307578103
2025-05-22 11:57:59,643 - train(rank0) - INFO -     loss_cont      : 0.0703761617342631
2025-05-22 11:57:59,643 - train(rank0) - INFO -     loss_uncer     : 0.07201737960179647
2025-05-22 11:57:59,657 - train(rank0) - INFO - Epoch - 5
2025-05-22 11:58:02,322 - train(rank0) - INFO - lr[0]: 0.000094 / lr[1]: 0.000940 / lr[2]: 0.000940
2025-05-22 11:58:02,322 - train(rank0) - INFO - [0/12]
2025-05-22 11:58:03,526 - train(rank0) - INFO - [2/12]
2025-05-22 11:58:04,767 - train(rank0) - INFO - [4/12]
2025-05-22 11:58:05,943 - train(rank0) - INFO - [6/12]
2025-05-22 11:58:07,097 - train(rank0) - INFO - [8/12]
2025-05-22 11:58:08,280 - train(rank0) - INFO - [10/12]
2025-05-22 11:58:09,233 - train(rank0) - INFO -     epoch          : 5
2025-05-22 11:58:09,234 - train(rank0) - INFO -     loss           : 0.18863463401794434
2025-05-22 11:58:09,234 - train(rank0) - INFO -     loss_mbce      : 0.042364812921732664
2025-05-22 11:58:09,234 - train(rank0) - INFO -     loss_pkd       : 0.006902622408233583
2025-05-22 11:58:09,235 - train(rank0) - INFO -     loss_cont      : 0.06612287213404974
2025-05-22 11:58:09,235 - train(rank0) - INFO -     loss_uncer     : 0.07324432333310445
2025-05-22 11:58:09,316 - train(rank0) - INFO - Epoch - 6
2025-05-22 11:58:11,924 - train(rank0) - INFO - lr[0]: 0.000092 / lr[1]: 0.000925 / lr[2]: 0.000925
2025-05-22 11:58:11,924 - train(rank0) - INFO - [0/12]
2025-05-22 11:58:13,108 - train(rank0) - INFO - [2/12]
2025-05-22 11:58:14,265 - train(rank0) - INFO - [4/12]
2025-05-22 11:58:15,441 - train(rank0) - INFO - [6/12]
2025-05-22 11:58:16,652 - train(rank0) - INFO - [8/12]
2025-05-22 11:58:17,821 - train(rank0) - INFO - [10/12]
2025-05-22 11:58:18,833 - train(rank0) - INFO -     epoch          : 6
2025-05-22 11:58:18,833 - train(rank0) - INFO -     loss           : 0.18338705350955328
2025-05-22 11:58:18,834 - train(rank0) - INFO -     loss_mbce      : 0.04461015062406659
2025-05-22 11:58:18,834 - train(rank0) - INFO -     loss_pkd       : 0.005857523428858258
2025-05-22 11:58:18,834 - train(rank0) - INFO -     loss_cont      : 0.06392880032459895
2025-05-22 11:58:18,834 - train(rank0) - INFO -     loss_uncer     : 0.0689905767639478
2025-05-22 11:58:18,844 - train(rank0) - INFO - Epoch - 7
2025-05-22 11:58:21,476 - train(rank0) - INFO - lr[0]: 0.000091 / lr[1]: 0.000910 / lr[2]: 0.000910
2025-05-22 11:58:21,477 - train(rank0) - INFO - [0/12]
2025-05-22 11:58:22,653 - train(rank0) - INFO - [2/12]
2025-05-22 11:58:23,795 - train(rank0) - INFO - [4/12]
2025-05-22 11:58:24,983 - train(rank0) - INFO - [6/12]
2025-05-22 11:58:26,167 - train(rank0) - INFO - [8/12]
2025-05-22 11:58:27,379 - train(rank0) - INFO - [10/12]
2025-05-22 11:58:28,326 - train(rank0) - INFO -     epoch          : 7
2025-05-22 11:58:28,327 - train(rank0) - INFO -     loss           : 0.19474990790088972
2025-05-22 11:58:28,327 - train(rank0) - INFO -     loss_mbce      : 0.045722722075879574
2025-05-22 11:58:28,327 - train(rank0) - INFO -     loss_pkd       : 0.0082315138085202
2025-05-22 11:58:28,327 - train(rank0) - INFO -     loss_cont      : 0.0714150369167328
2025-05-22 11:58:28,328 - train(rank0) - INFO -     loss_uncer     : 0.06938063402970633
2025-05-22 11:58:28,373 - train(rank0) - INFO - Epoch - 8
2025-05-22 11:58:30,999 - train(rank0) - INFO - lr[0]: 0.000089 / lr[1]: 0.000894 / lr[2]: 0.000894
2025-05-22 11:58:30,999 - train(rank0) - INFO - [0/12]
2025-05-22 11:58:32,192 - train(rank0) - INFO - [2/12]
2025-05-22 11:58:33,370 - train(rank0) - INFO - [4/12]
2025-05-22 11:58:34,575 - train(rank0) - INFO - [6/12]
2025-05-22 11:58:35,781 - train(rank0) - INFO - [8/12]
2025-05-22 11:58:36,948 - train(rank0) - INFO - [10/12]
2025-05-22 11:58:37,986 - train(rank0) - INFO -     epoch          : 8
2025-05-22 11:58:37,987 - train(rank0) - INFO -     loss           : 0.19075436145067215
2025-05-22 11:58:37,988 - train(rank0) - INFO -     loss_mbce      : 0.040347942151129246
2025-05-22 11:58:37,989 - train(rank0) - INFO -     loss_pkd       : 0.00903555018642995
2025-05-22 11:58:37,989 - train(rank0) - INFO -     loss_cont      : 0.06950613607962924
2025-05-22 11:58:37,989 - train(rank0) - INFO -     loss_uncer     : 0.07186473061641058
2025-05-22 11:58:38,046 - train(rank0) - INFO - Epoch - 9
2025-05-22 11:58:40,625 - train(rank0) - INFO - lr[0]: 0.000088 / lr[1]: 0.000879 / lr[2]: 0.000879
2025-05-22 11:58:40,625 - train(rank0) - INFO - [0/12]
2025-05-22 11:58:41,811 - train(rank0) - INFO - [2/12]
2025-05-22 11:58:43,000 - train(rank0) - INFO - [4/12]
2025-05-22 11:58:44,189 - train(rank0) - INFO - [6/12]
2025-05-22 11:58:45,370 - train(rank0) - INFO - [8/12]
2025-05-22 11:58:46,528 - train(rank0) - INFO - [10/12]
2025-05-22 11:58:47,499 - train(rank0) - INFO -     epoch          : 9
2025-05-22 11:58:47,500 - train(rank0) - INFO -     loss           : 0.17707273984948793
2025-05-22 11:58:47,500 - train(rank0) - INFO -     loss_mbce      : 0.03726604782665769
2025-05-22 11:58:47,500 - train(rank0) - INFO -     loss_pkd       : 0.005953435866103973
2025-05-22 11:58:47,500 - train(rank0) - INFO -     loss_cont      : 0.06517840822537739
2025-05-22 11:58:47,500 - train(rank0) - INFO -     loss_uncer     : 0.06867484748363495
2025-05-22 11:58:47,550 - train(rank0) - INFO - Epoch - 10
2025-05-22 11:58:50,122 - train(rank0) - INFO - lr[0]: 0.000086 / lr[1]: 0.000864 / lr[2]: 0.000864
2025-05-22 11:58:50,123 - train(rank0) - INFO - [0/12]
2025-05-22 11:58:51,281 - train(rank0) - INFO - [2/12]
2025-05-22 11:58:52,485 - train(rank0) - INFO - [4/12]
2025-05-22 11:58:53,646 - train(rank0) - INFO - [6/12]
2025-05-22 11:58:54,839 - train(rank0) - INFO - [8/12]
2025-05-22 11:58:56,015 - train(rank0) - INFO - [10/12]
2025-05-22 11:58:57,102 - train(rank0) - INFO - Number of val loader: 57
2025-05-22 11:59:01,291 - train(rank0) - INFO -     epoch          : 10
2025-05-22 11:59:01,292 - train(rank0) - INFO -     loss           : 0.1825631447136402
2025-05-22 11:59:01,292 - train(rank0) - INFO -     loss_mbce      : 0.03925013619785508
2025-05-22 11:59:01,292 - train(rank0) - INFO -     loss_pkd       : 0.013807716325876148
2025-05-22 11:59:01,292 - train(rank0) - INFO -     loss_cont      : 0.06299806634585063
2025-05-22 11:59:01,292 - train(rank0) - INFO -     loss_uncer     : 0.0665072207649549
2025-05-22 11:59:01,292 - train(rank0) - INFO -     val_Pixel_Accuracy_old: 98.59
2025-05-22 11:59:01,292 - train(rank0) - INFO -     val_Pixel_Accuracy_new: 59.98
2025-05-22 11:59:01,292 - train(rank0) - INFO -     val_Pixel_Accuracy_harmonic: 74.58
2025-05-22 11:59:01,293 - train(rank0) - INFO -     val_Pixel_Accuracy_overall: 89.60
2025-05-22 11:59:01,293 - train(rank0) - INFO -     val_Pixel_Accuracy_Class_old: 98.59
2025-05-22 11:59:01,293 - train(rank0) - INFO -     val_Pixel_Accuracy_Class_new: 59.98
2025-05-22 11:59:01,293 - train(rank0) - INFO -     val_Pixel_Accuracy_Class_harmonic: 74.58
2025-05-22 11:59:01,293 - train(rank0) - INFO -     val_Pixel_Accuracy_Class_overall: 79.28
2025-05-22 11:59:01,293 - train(rank0) - INFO -     val_Pixel_Accuracy_Class_by_class: 
 0  background 98.59
17 *sheep 59.98

2025-05-22 11:59:01,293 - train(rank0) - INFO -     val_Mean_Intersection_over_Union_old: 92.59
2025-05-22 11:59:01,293 - train(rank0) - INFO -     val_Mean_Intersection_over_Union_new: 59.86
2025-05-22 11:59:01,293 - train(rank0) - INFO -     val_Mean_Intersection_over_Union_harmonic: 72.71
2025-05-22 11:59:01,293 - train(rank0) - INFO -     val_Mean_Intersection_over_Union_overall: 76.23
2025-05-22 11:59:01,293 - train(rank0) - INFO -     val_Mean_Intersection_over_Union_by_class: 
 0  background 92.59
17 *sheep 59.86

2025-05-22 11:59:01,935 - train(rank0) - INFO - Saving checkpoint: saved_voc/models/overlap_15-1_Adapter/step_2/checkpoint-epoch60.pth ...
2025-05-22 11:59:01,936 - train(rank0) - INFO - computing prototypes...
2025-05-22 11:59:03,832 - train(rank0) - INFO - [0/12]
2025-05-22 11:59:04,076 - train(rank0) - INFO - [2/12]
2025-05-22 11:59:04,319 - train(rank0) - INFO - [4/12]
2025-05-22 11:59:04,562 - train(rank0) - INFO - [6/12]
2025-05-22 11:59:04,805 - train(rank0) - INFO - [8/12]
2025-05-22 11:59:05,049 - train(rank0) - INFO - [10/12]
2025-05-22 11:59:05,494 - train(rank0) - INFO - computing noise...
2025-05-22 11:59:07,409 - train(rank0) - INFO - [0/12]
2025-05-22 11:59:07,660 - train(rank0) - INFO - [2/12]
2025-05-22 11:59:07,909 - train(rank0) - INFO - [4/12]
2025-05-22 11:59:08,154 - train(rank0) - INFO - [6/12]
2025-05-22 11:59:08,400 - train(rank0) - INFO - [8/12]
2025-05-22 11:59:08,646 - train(rank0) - INFO - [10/12]
2025-05-22 11:59:09,119 - train(rank0) - INFO - Epoch - 11
2025-05-22 11:59:11,754 - train(rank0) - INFO - lr[0]: 0.000085 / lr[1]: 0.000849 / lr[2]: 0.000849
2025-05-22 11:59:11,754 - train(rank0) - INFO - [0/12]
2025-05-22 11:59:12,973 - train(rank0) - INFO - [2/12]
2025-05-22 11:59:14,126 - train(rank0) - INFO - [4/12]
2025-05-22 11:59:15,308 - train(rank0) - INFO - [6/12]
2025-05-22 11:59:16,476 - train(rank0) - INFO - [8/12]
2025-05-22 11:59:17,669 - train(rank0) - INFO - [10/12]
2025-05-22 11:59:18,711 - train(rank0) - INFO -     epoch          : 11
2025-05-22 11:59:18,712 - train(rank0) - INFO -     loss           : 0.18241526807347933
2025-05-22 11:59:18,712 - train(rank0) - INFO -     loss_mbce      : 0.039845809961358704
2025-05-22 11:59:18,713 - train(rank0) - INFO -     loss_pkd       : 0.008831956996194398
2025-05-22 11:59:18,713 - train(rank0) - INFO -     loss_cont      : 0.06717521448930104
2025-05-22 11:59:18,713 - train(rank0) - INFO -     loss_uncer     : 0.06656228701273599
2025-05-22 11:59:18,721 - train(rank0) - INFO - Epoch - 12
2025-05-22 11:59:21,378 - train(rank0) - INFO - lr[0]: 0.000083 / lr[1]: 0.000833 / lr[2]: 0.000833
2025-05-22 11:59:21,378 - train(rank0) - INFO - [0/12]
2025-05-22 11:59:22,571 - train(rank0) - INFO - [2/12]
2025-05-22 11:59:23,769 - train(rank0) - INFO - [4/12]
2025-05-22 11:59:24,934 - train(rank0) - INFO - [6/12]
2025-05-22 11:59:26,098 - train(rank0) - INFO - [8/12]
2025-05-22 11:59:27,313 - train(rank0) - INFO - [10/12]
2025-05-22 11:59:28,316 - train(rank0) - INFO -     epoch          : 12
2025-05-22 11:59:28,317 - train(rank0) - INFO -     loss           : 0.169939915339152
2025-05-22 11:59:28,317 - train(rank0) - INFO -     loss_mbce      : 0.03674739180132747
2025-05-22 11:59:28,317 - train(rank0) - INFO -     loss_pkd       : 0.003796110686380416
2025-05-22 11:59:28,317 - train(rank0) - INFO -     loss_cont      : 0.06275823960701625
2025-05-22 11:59:28,317 - train(rank0) - INFO -     loss_uncer     : 0.06663817316293717
2025-05-22 11:59:28,328 - train(rank0) - INFO - Epoch - 13
2025-05-22 11:59:31,009 - train(rank0) - INFO - lr[0]: 0.000082 / lr[1]: 0.000818 / lr[2]: 0.000818
2025-05-22 11:59:31,009 - train(rank0) - INFO - [0/12]
2025-05-22 11:59:32,197 - train(rank0) - INFO - [2/12]
2025-05-22 11:59:33,402 - train(rank0) - INFO - [4/12]
2025-05-22 11:59:34,612 - train(rank0) - INFO - [6/12]
2025-05-22 11:59:35,826 - train(rank0) - INFO - [8/12]
2025-05-22 11:59:36,991 - train(rank0) - INFO - [10/12]
2025-05-22 11:59:37,962 - train(rank0) - INFO -     epoch          : 13
2025-05-22 11:59:37,963 - train(rank0) - INFO -     loss           : 0.18735976765553156
2025-05-22 11:59:37,963 - train(rank0) - INFO -     loss_mbce      : 0.04224537964910269
2025-05-22 11:59:37,963 - train(rank0) - INFO -     loss_pkd       : 0.014872557088286461
2025-05-22 11:59:37,964 - train(rank0) - INFO -     loss_cont      : 0.06493437141180039
2025-05-22 11:59:37,964 - train(rank0) - INFO -     loss_uncer     : 0.0653074582417806
2025-05-22 11:59:38,052 - train(rank0) - INFO - Epoch - 14
2025-05-22 11:59:40,787 - train(rank0) - INFO - lr[0]: 0.000080 / lr[1]: 0.000803 / lr[2]: 0.000803
2025-05-22 11:59:40,787 - train(rank0) - INFO - [0/12]
2025-05-22 11:59:41,996 - train(rank0) - INFO - [2/12]
2025-05-22 11:59:43,171 - train(rank0) - INFO - [4/12]
2025-05-22 11:59:44,363 - train(rank0) - INFO - [6/12]
2025-05-22 11:59:45,525 - train(rank0) - INFO - [8/12]
2025-05-22 11:59:46,715 - train(rank0) - INFO - [10/12]
2025-05-22 11:59:47,766 - train(rank0) - INFO -     epoch          : 14
2025-05-22 11:59:47,767 - train(rank0) - INFO -     loss           : 0.17151000971595445
2025-05-22 11:59:47,767 - train(rank0) - INFO -     loss_mbce      : 0.0354846753180027
2025-05-22 11:59:47,767 - train(rank0) - INFO -     loss_pkd       : 0.00574069107339407
2025-05-22 11:59:47,767 - train(rank0) - INFO -     loss_cont      : 0.06273040374120077
2025-05-22 11:59:47,767 - train(rank0) - INFO -     loss_uncer     : 0.06755423694849015
2025-05-22 11:59:47,783 - train(rank0) - INFO - Epoch - 15
2025-05-22 11:59:50,293 - train(rank0) - INFO - lr[0]: 0.000079 / lr[1]: 0.000787 / lr[2]: 0.000787
2025-05-22 11:59:50,294 - train(rank0) - INFO - [0/12]
2025-05-22 11:59:51,497 - train(rank0) - INFO - [2/12]
2025-05-22 11:59:52,713 - train(rank0) - INFO - [4/12]
2025-05-22 11:59:53,924 - train(rank0) - INFO - [6/12]
2025-05-22 11:59:55,136 - train(rank0) - INFO - [8/12]
2025-05-22 11:59:56,355 - train(rank0) - INFO - [10/12]
2025-05-22 11:59:57,323 - train(rank0) - INFO -     epoch          : 15
2025-05-22 11:59:57,324 - train(rank0) - INFO -     loss           : 0.1714401418964068
2025-05-22 11:59:57,324 - train(rank0) - INFO -     loss_mbce      : 0.030548467456052702
2025-05-22 11:59:57,324 - train(rank0) - INFO -     loss_pkd       : 0.004796114784160939
2025-05-22 11:59:57,325 - train(rank0) - INFO -     loss_cont      : 0.06332517862319947
2025-05-22 11:59:57,325 - train(rank0) - INFO -     loss_uncer     : 0.07277038047711054
2025-05-22 11:59:57,340 - train(rank0) - INFO - Epoch - 16
2025-05-22 12:00:00,011 - train(rank0) - INFO - lr[0]: 0.000077 / lr[1]: 0.000772 / lr[2]: 0.000772
2025-05-22 12:00:00,012 - train(rank0) - INFO - [0/12]
2025-05-22 12:00:01,201 - train(rank0) - INFO - [2/12]
2025-05-22 12:00:02,401 - train(rank0) - INFO - [4/12]
2025-05-22 12:00:03,617 - train(rank0) - INFO - [6/12]
2025-05-22 12:00:04,795 - train(rank0) - INFO - [8/12]
2025-05-22 12:00:05,965 - train(rank0) - INFO - [10/12]
2025-05-22 12:00:06,988 - train(rank0) - INFO -     epoch          : 16
2025-05-22 12:00:06,989 - train(rank0) - INFO -     loss           : 0.16954904422163963
2025-05-22 12:00:06,989 - train(rank0) - INFO -     loss_mbce      : 0.031758238561451435
2025-05-22 12:00:06,989 - train(rank0) - INFO -     loss_pkd       : 0.005191375069747058
2025-05-22 12:00:06,989 - train(rank0) - INFO -     loss_cont      : 0.06317010074853897
2025-05-22 12:00:06,990 - train(rank0) - INFO -     loss_uncer     : 0.06942932903766631
2025-05-22 12:00:07,011 - train(rank0) - INFO - Epoch - 17
2025-05-22 12:00:09,720 - train(rank0) - INFO - lr[0]: 0.000076 / lr[1]: 0.000756 / lr[2]: 0.000756
2025-05-22 12:00:09,720 - train(rank0) - INFO - [0/12]
2025-05-22 12:00:10,896 - train(rank0) - INFO - [2/12]
2025-05-22 12:00:12,113 - train(rank0) - INFO - [4/12]
2025-05-22 12:00:13,275 - train(rank0) - INFO - [6/12]
2025-05-22 12:00:14,471 - train(rank0) - INFO - [8/12]
2025-05-22 12:00:15,692 - train(rank0) - INFO - [10/12]
2025-05-22 12:00:16,684 - train(rank0) - INFO -     epoch          : 17
2025-05-22 12:00:16,685 - train(rank0) - INFO -     loss           : 0.17156695077816644
2025-05-22 12:00:16,685 - train(rank0) - INFO -     loss_mbce      : 0.034116636806478105
2025-05-22 12:00:16,686 - train(rank0) - INFO -     loss_pkd       : 0.007609752089289638
2025-05-22 12:00:16,686 - train(rank0) - INFO -     loss_cont      : 0.06407304555177688
2025-05-22 12:00:16,686 - train(rank0) - INFO -     loss_uncer     : 0.06576751445730528
2025-05-22 12:00:16,696 - train(rank0) - INFO - Epoch - 18
2025-05-22 12:00:19,419 - train(rank0) - INFO - lr[0]: 0.000074 / lr[1]: 0.000741 / lr[2]: 0.000741
2025-05-22 12:00:19,419 - train(rank0) - INFO - [0/12]
2025-05-22 12:00:20,562 - train(rank0) - INFO - [2/12]
2025-05-22 12:00:21,773 - train(rank0) - INFO - [4/12]
2025-05-22 12:00:23,019 - train(rank0) - INFO - [6/12]
2025-05-22 12:00:24,216 - train(rank0) - INFO - [8/12]
2025-05-22 12:00:25,385 - train(rank0) - INFO - [10/12]
2025-05-22 12:00:26,377 - train(rank0) - INFO -     epoch          : 18
2025-05-22 12:00:26,377 - train(rank0) - INFO -     loss           : 0.16448999692996344
2025-05-22 12:00:26,378 - train(rank0) - INFO -     loss_mbce      : 0.028987736130754154
2025-05-22 12:00:26,378 - train(rank0) - INFO -     loss_pkd       : 0.004041641501923247
2025-05-22 12:00:26,378 - train(rank0) - INFO -     loss_cont      : 0.06013780186573666
2025-05-22 12:00:26,378 - train(rank0) - INFO -     loss_uncer     : 0.07132281760374706
2025-05-22 12:00:26,425 - train(rank0) - INFO - Epoch - 19
2025-05-22 12:00:28,972 - train(rank0) - INFO - lr[0]: 0.000073 / lr[1]: 0.000725 / lr[2]: 0.000725
2025-05-22 12:00:28,973 - train(rank0) - INFO - [0/12]
2025-05-22 12:00:30,185 - train(rank0) - INFO - [2/12]
2025-05-22 12:00:31,397 - train(rank0) - INFO - [4/12]
2025-05-22 12:00:32,588 - train(rank0) - INFO - [6/12]
2025-05-22 12:00:33,751 - train(rank0) - INFO - [8/12]
2025-05-22 12:00:34,942 - train(rank0) - INFO - [10/12]
2025-05-22 12:00:35,959 - train(rank0) - INFO -     epoch          : 19
2025-05-22 12:00:35,960 - train(rank0) - INFO -     loss           : 0.17348830526073775
2025-05-22 12:00:35,960 - train(rank0) - INFO -     loss_mbce      : 0.033412342968707286
2025-05-22 12:00:35,961 - train(rank0) - INFO -     loss_pkd       : 0.008802775967827378
2025-05-22 12:00:35,961 - train(rank0) - INFO -     loss_cont      : 0.06258780608574549
2025-05-22 12:00:35,961 - train(rank0) - INFO -     loss_uncer     : 0.06868538061777753
2025-05-22 12:00:35,970 - train(rank0) - INFO - Epoch - 20
2025-05-22 12:00:38,546 - train(rank0) - INFO - lr[0]: 0.000071 / lr[1]: 0.000710 / lr[2]: 0.000710
2025-05-22 12:00:38,546 - train(rank0) - INFO - [0/12]
2025-05-22 12:00:39,721 - train(rank0) - INFO - [2/12]
2025-05-22 12:00:40,923 - train(rank0) - INFO - [4/12]
2025-05-22 12:00:42,113 - train(rank0) - INFO - [6/12]
2025-05-22 12:00:43,325 - train(rank0) - INFO - [8/12]
2025-05-22 12:00:44,541 - train(rank0) - INFO - [10/12]
2025-05-22 12:00:45,589 - train(rank0) - INFO - Number of val loader: 57
2025-05-22 12:00:49,018 - train(rank0) - INFO -     epoch          : 20
2025-05-22 12:00:49,019 - train(rank0) - INFO -     loss           : 0.1711364264289538
2025-05-22 12:00:49,019 - train(rank0) - INFO -     loss_mbce      : 0.03324172405215601
2025-05-22 12:00:49,020 - train(rank0) - INFO -     loss_pkd       : 0.0037288677655548477
2025-05-22 12:00:49,020 - train(rank0) - INFO -     loss_cont      : 0.06675815681616465
2025-05-22 12:00:49,020 - train(rank0) - INFO -     loss_uncer     : 0.0674076775709788
2025-05-22 12:00:49,020 - train(rank0) - INFO -     val_Pixel_Accuracy_old: 98.54
2025-05-22 12:00:49,020 - train(rank0) - INFO -     val_Pixel_Accuracy_new: 66.76
2025-05-22 12:00:49,020 - train(rank0) - INFO -     val_Pixel_Accuracy_harmonic: 79.59
2025-05-22 12:00:49,020 - train(rank0) - INFO -     val_Pixel_Accuracy_overall: 91.15
2025-05-22 12:00:49,020 - train(rank0) - INFO -     val_Pixel_Accuracy_Class_old: 98.54
2025-05-22 12:00:49,021 - train(rank0) - INFO -     val_Pixel_Accuracy_Class_new: 66.76
2025-05-22 12:00:49,021 - train(rank0) - INFO -     val_Pixel_Accuracy_Class_harmonic: 79.59
2025-05-22 12:00:49,021 - train(rank0) - INFO -     val_Pixel_Accuracy_Class_overall: 82.65
2025-05-22 12:00:49,021 - train(rank0) - INFO -     val_Pixel_Accuracy_Class_by_class: 
 0  background 98.54
17 *sheep 66.76

2025-05-22 12:00:49,021 - train(rank0) - INFO -     val_Mean_Intersection_over_Union_old: 93.72
2025-05-22 12:00:49,021 - train(rank0) - INFO -     val_Mean_Intersection_over_Union_new: 66.53
2025-05-22 12:00:49,021 - train(rank0) - INFO -     val_Mean_Intersection_over_Union_harmonic: 77.82
2025-05-22 12:00:49,021 - train(rank0) - INFO -     val_Mean_Intersection_over_Union_overall: 80.12
2025-05-22 12:00:49,022 - train(rank0) - INFO -     val_Mean_Intersection_over_Union_by_class: 
 0  background 93.72
17 *sheep 66.53

2025-05-22 12:00:49,786 - train(rank0) - INFO - Saving checkpoint: saved_voc/models/overlap_15-1_Adapter/step_2/checkpoint-epoch60.pth ...
2025-05-22 12:00:49,787 - train(rank0) - INFO - computing prototypes...
2025-05-22 12:00:51,738 - train(rank0) - INFO - [0/12]
2025-05-22 12:00:51,982 - train(rank0) - INFO - [2/12]
2025-05-22 12:00:52,227 - train(rank0) - INFO - [4/12]
2025-05-22 12:00:52,470 - train(rank0) - INFO - [6/12]
2025-05-22 12:00:52,713 - train(rank0) - INFO - [8/12]
2025-05-22 12:00:52,957 - train(rank0) - INFO - [10/12]
2025-05-22 12:00:53,384 - train(rank0) - INFO - computing noise...
2025-05-22 12:00:55,256 - train(rank0) - INFO - [0/12]
2025-05-22 12:00:55,510 - train(rank0) - INFO - [2/12]
2025-05-22 12:00:55,757 - train(rank0) - INFO - [4/12]
2025-05-22 12:00:56,004 - train(rank0) - INFO - [6/12]
2025-05-22 12:00:56,250 - train(rank0) - INFO - [8/12]
2025-05-22 12:00:56,497 - train(rank0) - INFO - [10/12]
2025-05-22 12:00:56,990 - train(rank0) - INFO - Epoch - 21
2025-05-22 12:00:59,719 - train(rank0) - INFO - lr[0]: 0.000069 / lr[1]: 0.000694 / lr[2]: 0.000694
2025-05-22 12:00:59,720 - train(rank0) - INFO - [0/12]
2025-05-22 12:01:00,916 - train(rank0) - INFO - [2/12]
2025-05-22 12:01:02,133 - train(rank0) - INFO - [4/12]
2025-05-22 12:01:03,309 - train(rank0) - INFO - [6/12]
2025-05-22 12:01:04,518 - train(rank0) - INFO - [8/12]
2025-05-22 12:01:05,681 - train(rank0) - INFO - [10/12]
2025-05-22 12:01:06,691 - train(rank0) - INFO -     epoch          : 21
2025-05-22 12:01:06,691 - train(rank0) - INFO -     loss           : 0.16383601600925127
2025-05-22 12:01:06,692 - train(rank0) - INFO -     loss_mbce      : 0.02630925038829446
2025-05-22 12:01:06,692 - train(rank0) - INFO -     loss_pkd       : 0.006143151767901145
2025-05-22 12:01:06,692 - train(rank0) - INFO -     loss_cont      : 0.06034071495135626
2025-05-22 12:01:06,692 - train(rank0) - INFO -     loss_uncer     : 0.07104289482037225
2025-05-22 12:01:06,703 - train(rank0) - INFO - Epoch - 22
2025-05-22 12:01:09,398 - train(rank0) - INFO - lr[0]: 0.000068 / lr[1]: 0.000679 / lr[2]: 0.000679
2025-05-22 12:01:09,398 - train(rank0) - INFO - [0/12]
2025-05-22 12:01:10,554 - train(rank0) - INFO - [2/12]
2025-05-22 12:01:11,755 - train(rank0) - INFO - [4/12]
2025-05-22 12:01:12,950 - train(rank0) - INFO - [6/12]
2025-05-22 12:01:14,162 - train(rank0) - INFO - [8/12]
2025-05-22 12:01:15,325 - train(rank0) - INFO - [10/12]
2025-05-22 12:01:16,306 - train(rank0) - INFO -     epoch          : 22
2025-05-22 12:01:16,307 - train(rank0) - INFO -     loss           : 0.17164850855867067
2025-05-22 12:01:16,307 - train(rank0) - INFO -     loss_mbce      : 0.030203229902933042
2025-05-22 12:01:16,308 - train(rank0) - INFO -     loss_pkd       : 0.006203309433961597
2025-05-22 12:01:16,308 - train(rank0) - INFO -     loss_cont      : 0.06297392696142197
2025-05-22 12:01:16,308 - train(rank0) - INFO -     loss_uncer     : 0.07226803898811342
2025-05-22 12:01:16,381 - train(rank0) - INFO - Epoch - 23
2025-05-22 12:01:19,069 - train(rank0) - INFO - lr[0]: 0.000066 / lr[1]: 0.000663 / lr[2]: 0.000663
2025-05-22 12:01:19,069 - train(rank0) - INFO - [0/12]
2025-05-22 12:01:20,262 - train(rank0) - INFO - [2/12]
2025-05-22 12:01:21,474 - train(rank0) - INFO - [4/12]
2025-05-22 12:01:22,664 - train(rank0) - INFO - [6/12]
2025-05-22 12:01:23,869 - train(rank0) - INFO - [8/12]
2025-05-22 12:01:25,066 - train(rank0) - INFO - [10/12]
2025-05-22 12:01:26,021 - train(rank0) - INFO -     epoch          : 23
2025-05-22 12:01:26,021 - train(rank0) - INFO -     loss           : 0.16613443940877914
2025-05-22 12:01:26,022 - train(rank0) - INFO -     loss_mbce      : 0.030177471227943897
2025-05-22 12:01:26,022 - train(rank0) - INFO -     loss_pkd       : 0.004360385986122613
2025-05-22 12:01:26,022 - train(rank0) - INFO -     loss_cont      : 0.0611121674378713
2025-05-22 12:01:26,022 - train(rank0) - INFO -     loss_uncer     : 0.07048441022634507
2025-05-22 12:01:26,063 - train(rank0) - INFO - Epoch - 24
2025-05-22 12:01:28,703 - train(rank0) - INFO - lr[0]: 0.000065 / lr[1]: 0.000647 / lr[2]: 0.000647
2025-05-22 12:01:28,703 - train(rank0) - INFO - [0/12]
2025-05-22 12:01:29,891 - train(rank0) - INFO - [2/12]
2025-05-22 12:01:31,028 - train(rank0) - INFO - [4/12]
2025-05-22 12:01:32,242 - train(rank0) - INFO - [6/12]
2025-05-22 12:01:33,460 - train(rank0) - INFO - [8/12]
2025-05-22 12:01:34,660 - train(rank0) - INFO - [10/12]
2025-05-22 12:01:35,663 - train(rank0) - INFO -     epoch          : 24
2025-05-22 12:01:35,663 - train(rank0) - INFO -     loss           : 0.17313029741247496
2025-05-22 12:01:35,663 - train(rank0) - INFO -     loss_mbce      : 0.03190916792179147
2025-05-22 12:01:35,664 - train(rank0) - INFO -     loss_pkd       : 0.011296970308952345
2025-05-22 12:01:35,664 - train(rank0) - INFO -     loss_cont      : 0.060271035134792324
2025-05-22 12:01:35,664 - train(rank0) - INFO -     loss_uncer     : 0.06965311765670779
2025-05-22 12:01:35,701 - train(rank0) - INFO - Epoch - 25
2025-05-22 12:01:38,360 - train(rank0) - INFO - lr[0]: 0.000063 / lr[1]: 0.000631 / lr[2]: 0.000631
2025-05-22 12:01:38,360 - train(rank0) - INFO - [0/12]
2025-05-22 12:01:39,526 - train(rank0) - INFO - [2/12]
2025-05-22 12:01:40,751 - train(rank0) - INFO - [4/12]
2025-05-22 12:01:41,930 - train(rank0) - INFO - [6/12]
2025-05-22 12:01:43,098 - train(rank0) - INFO - [8/12]
2025-05-22 12:01:44,326 - train(rank0) - INFO - [10/12]
2025-05-22 12:01:45,299 - train(rank0) - INFO -     epoch          : 25
2025-05-22 12:01:45,300 - train(rank0) - INFO -     loss           : 0.16371090958515802
2025-05-22 12:01:45,300 - train(rank0) - INFO -     loss_mbce      : 0.027434236990908783
2025-05-22 12:01:45,300 - train(rank0) - INFO -     loss_pkd       : 0.005256717389177841
2025-05-22 12:01:45,301 - train(rank0) - INFO -     loss_cont      : 0.06102976351976394
2025-05-22 12:01:45,301 - train(rank0) - INFO -     loss_uncer     : 0.06999019036690393
2025-05-22 12:01:45,354 - train(rank0) - INFO - Epoch - 26
2025-05-22 12:01:48,026 - train(rank0) - INFO - lr[0]: 0.000062 / lr[1]: 0.000616 / lr[2]: 0.000616
2025-05-22 12:01:48,027 - train(rank0) - INFO - [0/12]
2025-05-22 12:01:49,218 - train(rank0) - INFO - [2/12]
2025-05-22 12:01:50,382 - train(rank0) - INFO - [4/12]
2025-05-22 12:01:51,562 - train(rank0) - INFO - [6/12]
2025-05-22 12:01:52,709 - train(rank0) - INFO - [8/12]
2025-05-22 12:01:53,941 - train(rank0) - INFO - [10/12]
2025-05-22 12:01:54,952 - train(rank0) - INFO -     epoch          : 26
2025-05-22 12:01:54,953 - train(rank0) - INFO -     loss           : 0.16226539264122644
2025-05-22 12:01:54,953 - train(rank0) - INFO -     loss_mbce      : 0.028539083587626617
2025-05-22 12:01:54,954 - train(rank0) - INFO -     loss_pkd       : 0.006633716050904089
2025-05-22 12:01:54,954 - train(rank0) - INFO -     loss_cont      : 0.06072314133246739
2025-05-22 12:01:54,954 - train(rank0) - INFO -     loss_uncer     : 0.06636945108572642
2025-05-22 12:01:54,963 - train(rank0) - INFO - Epoch - 27
2025-05-22 12:01:57,609 - train(rank0) - INFO - lr[0]: 0.000060 / lr[1]: 0.000600 / lr[2]: 0.000600
2025-05-22 12:01:57,609 - train(rank0) - INFO - [0/12]
2025-05-22 12:01:58,787 - train(rank0) - INFO - [2/12]
2025-05-22 12:01:59,948 - train(rank0) - INFO - [4/12]
2025-05-22 12:02:01,141 - train(rank0) - INFO - [6/12]
2025-05-22 12:02:02,311 - train(rank0) - INFO - [8/12]
2025-05-22 12:02:03,532 - train(rank0) - INFO - [10/12]
2025-05-22 12:02:04,558 - train(rank0) - INFO -     epoch          : 27
2025-05-22 12:02:04,559 - train(rank0) - INFO -     loss           : 0.17335299278299013
2025-05-22 12:02:04,559 - train(rank0) - INFO -     loss_mbce      : 0.03146289847791195
2025-05-22 12:02:04,559 - train(rank0) - INFO -     loss_pkd       : 0.005956730346952099
2025-05-22 12:02:04,559 - train(rank0) - INFO -     loss_cont      : 0.0649241601427396
2025-05-22 12:02:04,560 - train(rank0) - INFO -     loss_uncer     : 0.07100920329491298
2025-05-22 12:02:04,569 - train(rank0) - INFO - Epoch - 28
2025-05-22 12:02:07,299 - train(rank0) - INFO - lr[0]: 0.000058 / lr[1]: 0.000584 / lr[2]: 0.000584
2025-05-22 12:02:07,299 - train(rank0) - INFO - [0/12]
2025-05-22 12:02:08,476 - train(rank0) - INFO - [2/12]
2025-05-22 12:02:09,647 - train(rank0) - INFO - [4/12]
2025-05-22 12:02:10,812 - train(rank0) - INFO - [6/12]
2025-05-22 12:02:12,049 - train(rank0) - INFO - [8/12]
2025-05-22 12:02:13,241 - train(rank0) - INFO - [10/12]
2025-05-22 12:02:14,264 - train(rank0) - INFO -     epoch          : 28
2025-05-22 12:02:14,265 - train(rank0) - INFO -     loss           : 0.17097293337186178
2025-05-22 12:02:14,266 - train(rank0) - INFO -     loss_mbce      : 0.03179735907663902
2025-05-22 12:02:14,266 - train(rank0) - INFO -     loss_pkd       : 0.005800186700071208
2025-05-22 12:02:14,266 - train(rank0) - INFO -     loss_cont      : 0.06336178431908289
2025-05-22 12:02:14,266 - train(rank0) - INFO -     loss_uncer     : 0.07001360009113948
2025-05-22 12:02:14,291 - train(rank0) - INFO - Epoch - 29
2025-05-22 12:02:16,907 - train(rank0) - INFO - lr[0]: 0.000057 / lr[1]: 0.000568 / lr[2]: 0.000568
2025-05-22 12:02:16,907 - train(rank0) - INFO - [0/12]
2025-05-22 12:02:18,119 - train(rank0) - INFO - [2/12]
2025-05-22 12:02:19,319 - train(rank0) - INFO - [4/12]
2025-05-22 12:02:20,491 - train(rank0) - INFO - [6/12]
2025-05-22 12:02:21,692 - train(rank0) - INFO - [8/12]
2025-05-22 12:02:22,876 - train(rank0) - INFO - [10/12]
2025-05-22 12:02:23,874 - train(rank0) - INFO -     epoch          : 29
2025-05-22 12:02:23,875 - train(rank0) - INFO -     loss           : 0.17129463578263918
2025-05-22 12:02:23,875 - train(rank0) - INFO -     loss_mbce      : 0.028260035905987024
2025-05-22 12:02:23,875 - train(rank0) - INFO -     loss_pkd       : 0.010077353896728406
2025-05-22 12:02:23,875 - train(rank0) - INFO -     loss_cont      : 0.06109500775734585
2025-05-22 12:02:23,875 - train(rank0) - INFO -     loss_uncer     : 0.071862231194973
2025-05-22 12:02:23,879 - train(rank0) - INFO - Epoch - 30
2025-05-22 12:02:26,472 - train(rank0) - INFO - lr[0]: 0.000055 / lr[1]: 0.000552 / lr[2]: 0.000552
2025-05-22 12:02:26,472 - train(rank0) - INFO - [0/12]
2025-05-22 12:02:27,660 - train(rank0) - INFO - [2/12]
2025-05-22 12:02:28,800 - train(rank0) - INFO - [4/12]
2025-05-22 12:02:30,012 - train(rank0) - INFO - [6/12]
2025-05-22 12:02:31,215 - train(rank0) - INFO - [8/12]
2025-05-22 12:02:32,392 - train(rank0) - INFO - [10/12]
2025-05-22 12:02:33,391 - train(rank0) - INFO - Number of val loader: 57
2025-05-22 12:02:36,776 - train(rank0) - INFO -     epoch          : 30
2025-05-22 12:02:36,777 - train(rank0) - INFO -     loss           : 0.1671504465242227
2025-05-22 12:02:36,777 - train(rank0) - INFO -     loss_mbce      : 0.02934983605518937
2025-05-22 12:02:36,777 - train(rank0) - INFO -     loss_pkd       : 0.004910773095616605
2025-05-22 12:02:36,777 - train(rank0) - INFO -     loss_cont      : 0.0616193691889445
2025-05-22 12:02:36,777 - train(rank0) - INFO -     loss_uncer     : 0.07127046287059784
2025-05-22 12:02:36,777 - train(rank0) - INFO -     val_Pixel_Accuracy_old: 98.55
2025-05-22 12:02:36,777 - train(rank0) - INFO -     val_Pixel_Accuracy_new: 67.48
2025-05-22 12:02:36,777 - train(rank0) - INFO -     val_Pixel_Accuracy_harmonic: 80.11
2025-05-22 12:02:36,777 - train(rank0) - INFO -     val_Pixel_Accuracy_overall: 91.32
2025-05-22 12:02:36,778 - train(rank0) - INFO -     val_Pixel_Accuracy_Class_old: 98.55
2025-05-22 12:02:36,778 - train(rank0) - INFO -     val_Pixel_Accuracy_Class_new: 67.48
2025-05-22 12:02:36,778 - train(rank0) - INFO -     val_Pixel_Accuracy_Class_harmonic: 80.11
2025-05-22 12:02:36,778 - train(rank0) - INFO -     val_Pixel_Accuracy_Class_overall: 83.01
2025-05-22 12:02:36,778 - train(rank0) - INFO -     val_Pixel_Accuracy_Class_by_class: 
 0  background 98.55
17 *sheep 67.48

2025-05-22 12:02:36,778 - train(rank0) - INFO -     val_Mean_Intersection_over_Union_old: 93.77
2025-05-22 12:02:36,778 - train(rank0) - INFO -     val_Mean_Intersection_over_Union_new: 67.26
2025-05-22 12:02:36,778 - train(rank0) - INFO -     val_Mean_Intersection_over_Union_harmonic: 78.33
2025-05-22 12:02:36,778 - train(rank0) - INFO -     val_Mean_Intersection_over_Union_overall: 80.52
2025-05-22 12:02:36,778 - train(rank0) - INFO -     val_Mean_Intersection_over_Union_by_class: 
 0  background 93.77
17 *sheep 67.26

2025-05-22 12:02:37,477 - train(rank0) - INFO - Saving checkpoint: saved_voc/models/overlap_15-1_Adapter/step_2/checkpoint-epoch60.pth ...
2025-05-22 12:02:37,478 - train(rank0) - INFO - computing prototypes...
2025-05-22 12:02:39,422 - train(rank0) - INFO - [0/12]
2025-05-22 12:02:39,677 - train(rank0) - INFO - [2/12]
2025-05-22 12:02:39,921 - train(rank0) - INFO - [4/12]
2025-05-22 12:02:40,164 - train(rank0) - INFO - [6/12]
2025-05-22 12:02:40,408 - train(rank0) - INFO - [8/12]
2025-05-22 12:02:40,652 - train(rank0) - INFO - [10/12]
2025-05-22 12:02:41,161 - train(rank0) - INFO - computing noise...
2025-05-22 12:02:43,090 - train(rank0) - INFO - [0/12]
2025-05-22 12:02:43,356 - train(rank0) - INFO - [2/12]
2025-05-22 12:02:43,604 - train(rank0) - INFO - [4/12]
2025-05-22 12:02:43,855 - train(rank0) - INFO - [6/12]
2025-05-22 12:02:44,102 - train(rank0) - INFO - [8/12]
2025-05-22 12:02:44,350 - train(rank0) - INFO - [10/12]
2025-05-22 12:02:44,885 - train(rank0) - INFO - Epoch - 31
2025-05-22 12:02:47,642 - train(rank0) - INFO - lr[0]: 0.000054 / lr[1]: 0.000536 / lr[2]: 0.000536
2025-05-22 12:02:47,643 - train(rank0) - INFO - [0/12]
2025-05-22 12:02:48,838 - train(rank0) - INFO - [2/12]
2025-05-22 12:02:49,970 - train(rank0) - INFO - [4/12]
2025-05-22 12:02:51,156 - train(rank0) - INFO - [6/12]
2025-05-22 12:02:52,377 - train(rank0) - INFO - [8/12]
2025-05-22 12:02:53,602 - train(rank0) - INFO - [10/12]
2025-05-22 12:02:54,581 - train(rank0) - INFO -     epoch          : 31
2025-05-22 12:02:54,581 - train(rank0) - INFO -     loss           : 0.1660290757815043
2025-05-22 12:02:54,582 - train(rank0) - INFO -     loss_mbce      : 0.029528518673032522
2025-05-22 12:02:54,582 - train(rank0) - INFO -     loss_pkd       : 0.004974722032784484
2025-05-22 12:02:54,582 - train(rank0) - INFO -     loss_cont      : 0.06260909289121629
2025-05-22 12:02:54,582 - train(rank0) - INFO -     loss_uncer     : 0.06891673853000005
2025-05-22 12:02:54,623 - train(rank0) - INFO - Epoch - 32
2025-05-22 12:02:57,346 - train(rank0) - INFO - lr[0]: 0.000052 / lr[1]: 0.000520 / lr[2]: 0.000520
2025-05-22 12:02:57,346 - train(rank0) - INFO - [0/12]
2025-05-22 12:02:58,564 - train(rank0) - INFO - [2/12]
2025-05-22 12:02:59,706 - train(rank0) - INFO - [4/12]
2025-05-22 12:03:00,919 - train(rank0) - INFO - [6/12]
2025-05-22 12:03:02,079 - train(rank0) - INFO - [8/12]
2025-05-22 12:03:03,298 - train(rank0) - INFO - [10/12]
2025-05-22 12:03:04,339 - train(rank0) - INFO -     epoch          : 32
2025-05-22 12:03:04,340 - train(rank0) - INFO -     loss           : 0.1609722226858139
2025-05-22 12:03:04,340 - train(rank0) - INFO -     loss_mbce      : 0.02447714617786308
2025-05-22 12:03:04,341 - train(rank0) - INFO -     loss_pkd       : 0.005268165902331627
2025-05-22 12:03:04,341 - train(rank0) - INFO -     loss_cont      : 0.059281186262766516
2025-05-22 12:03:04,341 - train(rank0) - INFO -     loss_uncer     : 0.07194572438796361
2025-05-22 12:03:04,386 - train(rank0) - INFO - Epoch - 33
2025-05-22 12:03:07,056 - train(rank0) - INFO - lr[0]: 0.000050 / lr[1]: 0.000504 / lr[2]: 0.000504
2025-05-22 12:03:07,057 - train(rank0) - INFO - [0/12]
2025-05-22 12:03:08,268 - train(rank0) - INFO - [2/12]
2025-05-22 12:03:09,440 - train(rank0) - INFO - [4/12]
2025-05-22 12:03:10,615 - train(rank0) - INFO - [6/12]
2025-05-22 12:03:11,786 - train(rank0) - INFO - [8/12]
2025-05-22 12:03:12,957 - train(rank0) - INFO - [10/12]
2025-05-22 12:03:13,976 - train(rank0) - INFO -     epoch          : 33
2025-05-22 12:03:13,977 - train(rank0) - INFO -     loss           : 0.17076564207673073
2025-05-22 12:03:13,978 - train(rank0) - INFO -     loss_mbce      : 0.03510263212956488
2025-05-22 12:03:13,978 - train(rank0) - INFO -     loss_pkd       : 0.006946258848377814
2025-05-22 12:03:13,978 - train(rank0) - INFO -     loss_cont      : 0.06086574941873551
2025-05-22 12:03:13,978 - train(rank0) - INFO -     loss_uncer     : 0.06785099953413011
2025-05-22 12:03:13,989 - train(rank0) - INFO - Epoch - 34
2025-05-22 12:03:16,700 - train(rank0) - INFO - lr[0]: 0.000049 / lr[1]: 0.000487 / lr[2]: 0.000487
2025-05-22 12:03:16,700 - train(rank0) - INFO - [0/12]
2025-05-22 12:03:17,871 - train(rank0) - INFO - [2/12]
2025-05-22 12:03:19,082 - train(rank0) - INFO - [4/12]
2025-05-22 12:03:20,291 - train(rank0) - INFO - [6/12]
2025-05-22 12:03:21,445 - train(rank0) - INFO - [8/12]
2025-05-22 12:03:22,672 - train(rank0) - INFO - [10/12]
2025-05-22 12:03:23,670 - train(rank0) - INFO -     epoch          : 34
2025-05-22 12:03:23,671 - train(rank0) - INFO -     loss           : 0.170463465154171
2025-05-22 12:03:23,671 - train(rank0) - INFO -     loss_mbce      : 0.03026377991773188
2025-05-22 12:03:23,671 - train(rank0) - INFO -     loss_pkd       : 0.01024441725652044
2025-05-22 12:03:23,671 - train(rank0) - INFO -     loss_cont      : 0.059308911363283805
2025-05-22 12:03:23,671 - train(rank0) - INFO -     loss_uncer     : 0.07064635579784712
2025-05-22 12:03:23,714 - train(rank0) - INFO - Epoch - 35
2025-05-22 12:03:26,398 - train(rank0) - INFO - lr[0]: 0.000047 / lr[1]: 0.000471 / lr[2]: 0.000471
2025-05-22 12:03:26,399 - train(rank0) - INFO - [0/12]
2025-05-22 12:03:27,596 - train(rank0) - INFO - [2/12]
2025-05-22 12:03:28,787 - train(rank0) - INFO - [4/12]
2025-05-22 12:03:29,927 - train(rank0) - INFO - [6/12]
2025-05-22 12:03:31,101 - train(rank0) - INFO - [8/12]
2025-05-22 12:03:32,283 - train(rank0) - INFO - [10/12]
2025-05-22 12:03:33,280 - train(rank0) - INFO -     epoch          : 35
2025-05-22 12:03:33,281 - train(rank0) - INFO -     loss           : 0.17339988052845
2025-05-22 12:03:33,281 - train(rank0) - INFO -     loss_mbce      : 0.0324403866349409
2025-05-22 12:03:33,281 - train(rank0) - INFO -     loss_pkd       : 0.006953354143964437
2025-05-22 12:03:33,282 - train(rank0) - INFO -     loss_cont      : 0.0623644322156906
2025-05-22 12:03:33,282 - train(rank0) - INFO -     loss_uncer     : 0.07164170344670613
2025-05-22 12:03:33,295 - train(rank0) - INFO - Epoch - 36
2025-05-22 12:03:36,036 - train(rank0) - INFO - lr[0]: 0.000045 / lr[1]: 0.000455 / lr[2]: 0.000455
2025-05-22 12:03:36,036 - train(rank0) - INFO - [0/12]
2025-05-22 12:03:37,231 - train(rank0) - INFO - [2/12]
2025-05-22 12:03:38,430 - train(rank0) - INFO - [4/12]
2025-05-22 12:03:39,610 - train(rank0) - INFO - [6/12]
2025-05-22 12:03:40,824 - train(rank0) - INFO - [8/12]
2025-05-22 12:03:42,005 - train(rank0) - INFO - [10/12]
2025-05-22 12:03:43,024 - train(rank0) - INFO -     epoch          : 36
2025-05-22 12:03:43,025 - train(rank0) - INFO -     loss           : 0.1659545178214709
2025-05-22 12:03:43,025 - train(rank0) - INFO -     loss_mbce      : 0.027649759780615568
2025-05-22 12:03:43,025 - train(rank0) - INFO -     loss_pkd       : 0.0063925605597129715
2025-05-22 12:03:43,026 - train(rank0) - INFO -     loss_cont      : 0.06213847994804383
2025-05-22 12:03:43,026 - train(rank0) - INFO -     loss_uncer     : 0.06977371374766032
2025-05-22 12:03:43,036 - train(rank0) - INFO - Epoch - 37
2025-05-22 12:03:45,747 - train(rank0) - INFO - lr[0]: 0.000044 / lr[1]: 0.000438 / lr[2]: 0.000438
2025-05-22 12:03:45,747 - train(rank0) - INFO - [0/12]
2025-05-22 12:03:46,903 - train(rank0) - INFO - [2/12]
2025-05-22 12:03:48,120 - train(rank0) - INFO - [4/12]
2025-05-22 12:03:49,299 - train(rank0) - INFO - [6/12]
2025-05-22 12:03:50,481 - train(rank0) - INFO - [8/12]
2025-05-22 12:03:51,656 - train(rank0) - INFO - [10/12]
2025-05-22 12:03:52,592 - train(rank0) - INFO -     epoch          : 37
2025-05-22 12:03:52,593 - train(rank0) - INFO -     loss           : 0.16077155992388725
2025-05-22 12:03:52,593 - train(rank0) - INFO -     loss_mbce      : 0.025767468536893528
2025-05-22 12:03:52,593 - train(rank0) - INFO -     loss_pkd       : 0.0047163219763509305
2025-05-22 12:03:52,593 - train(rank0) - INFO -     loss_cont      : 0.05883689820766449
2025-05-22 12:03:52,593 - train(rank0) - INFO -     loss_uncer     : 0.07145086973905564
2025-05-22 12:03:52,664 - train(rank0) - INFO - Epoch - 38
2025-05-22 12:03:55,267 - train(rank0) - INFO - lr[0]: 0.000042 / lr[1]: 0.000422 / lr[2]: 0.000422
2025-05-22 12:03:55,268 - train(rank0) - INFO - [0/12]
2025-05-22 12:03:56,472 - train(rank0) - INFO - [2/12]
2025-05-22 12:03:57,645 - train(rank0) - INFO - [4/12]
2025-05-22 12:03:58,852 - train(rank0) - INFO - [6/12]
2025-05-22 12:04:00,032 - train(rank0) - INFO - [8/12]
2025-05-22 12:04:01,225 - train(rank0) - INFO - [10/12]
2025-05-22 12:04:02,214 - train(rank0) - INFO -     epoch          : 38
2025-05-22 12:04:02,215 - train(rank0) - INFO -     loss           : 0.1667496102551619
2025-05-22 12:04:02,215 - train(rank0) - INFO -     loss_mbce      : 0.025589934550225735
2025-05-22 12:04:02,215 - train(rank0) - INFO -     loss_pkd       : 0.00669378253708904
2025-05-22 12:04:02,216 - train(rank0) - INFO -     loss_cont      : 0.0631293088197708
2025-05-22 12:04:02,216 - train(rank0) - INFO -     loss_uncer     : 0.07133658329645794
2025-05-22 12:04:02,224 - train(rank0) - INFO - Epoch - 39
2025-05-22 12:04:04,828 - train(rank0) - INFO - lr[0]: 0.000041 / lr[1]: 0.000405 / lr[2]: 0.000405
2025-05-22 12:04:04,828 - train(rank0) - INFO - [0/12]
2025-05-22 12:04:06,018 - train(rank0) - INFO - [2/12]
2025-05-22 12:04:07,205 - train(rank0) - INFO - [4/12]
2025-05-22 12:04:08,400 - train(rank0) - INFO - [6/12]
2025-05-22 12:04:09,569 - train(rank0) - INFO - [8/12]
2025-05-22 12:04:10,759 - train(rank0) - INFO - [10/12]
2025-05-22 12:04:11,678 - train(rank0) - INFO -     epoch          : 39
2025-05-22 12:04:11,679 - train(rank0) - INFO -     loss           : 0.16263909017046294
2025-05-22 12:04:11,679 - train(rank0) - INFO -     loss_mbce      : 0.0264314953237772
2025-05-22 12:04:11,679 - train(rank0) - INFO -     loss_pkd       : 0.007285293140739668
2025-05-22 12:04:11,679 - train(rank0) - INFO -     loss_cont      : 0.06009728362162908
2025-05-22 12:04:11,680 - train(rank0) - INFO -     loss_uncer     : 0.06882501542568208
2025-05-22 12:04:11,778 - train(rank0) - INFO - Epoch - 40
2025-05-22 12:04:14,352 - train(rank0) - INFO - lr[0]: 0.000039 / lr[1]: 0.000389 / lr[2]: 0.000389
2025-05-22 12:04:14,352 - train(rank0) - INFO - [0/12]
2025-05-22 12:04:15,605 - train(rank0) - INFO - [2/12]
2025-05-22 12:04:16,784 - train(rank0) - INFO - [4/12]
2025-05-22 12:04:18,014 - train(rank0) - INFO - [6/12]
2025-05-22 12:04:19,237 - train(rank0) - INFO - [8/12]
2025-05-22 12:04:20,392 - train(rank0) - INFO - [10/12]
2025-05-22 12:04:21,427 - train(rank0) - INFO - Number of val loader: 57
2025-05-22 12:04:24,978 - train(rank0) - INFO -     epoch          : 40
2025-05-22 12:04:24,979 - train(rank0) - INFO -     loss           : 0.16417176897327104
2025-05-22 12:04:24,979 - train(rank0) - INFO -     loss_mbce      : 0.025497582430640858
2025-05-22 12:04:24,979 - train(rank0) - INFO -     loss_pkd       : 0.005135689546780971
2025-05-22 12:04:24,979 - train(rank0) - INFO -     loss_cont      : 0.059066604077816005
2025-05-22 12:04:24,979 - train(rank0) - INFO -     loss_uncer     : 0.0744718904296557
2025-05-22 12:04:24,979 - train(rank0) - INFO -     val_Pixel_Accuracy_old: 98.55
2025-05-22 12:04:24,979 - train(rank0) - INFO -     val_Pixel_Accuracy_new: 68.53
2025-05-22 12:04:24,980 - train(rank0) - INFO -     val_Pixel_Accuracy_harmonic: 80.84
2025-05-22 12:04:24,980 - train(rank0) - INFO -     val_Pixel_Accuracy_overall: 91.56
2025-05-22 12:04:24,980 - train(rank0) - INFO -     val_Pixel_Accuracy_Class_old: 98.55
2025-05-22 12:04:24,980 - train(rank0) - INFO -     val_Pixel_Accuracy_Class_new: 68.53
2025-05-22 12:04:24,980 - train(rank0) - INFO -     val_Pixel_Accuracy_Class_harmonic: 80.84
2025-05-22 12:04:24,980 - train(rank0) - INFO -     val_Pixel_Accuracy_Class_overall: 83.54
2025-05-22 12:04:24,980 - train(rank0) - INFO -     val_Pixel_Accuracy_Class_by_class: 
 0  background 98.55
17 *sheep 68.53

2025-05-22 12:04:24,980 - train(rank0) - INFO -     val_Mean_Intersection_over_Union_old: 93.94
2025-05-22 12:04:24,980 - train(rank0) - INFO -     val_Mean_Intersection_over_Union_new: 68.30
2025-05-22 12:04:24,980 - train(rank0) - INFO -     val_Mean_Intersection_over_Union_harmonic: 79.09
2025-05-22 12:04:24,980 - train(rank0) - INFO -     val_Mean_Intersection_over_Union_overall: 81.12
2025-05-22 12:04:24,980 - train(rank0) - INFO -     val_Mean_Intersection_over_Union_by_class: 
 0  background 93.94
17 *sheep 68.30

2025-05-22 12:04:25,679 - train(rank0) - INFO - Saving checkpoint: saved_voc/models/overlap_15-1_Adapter/step_2/checkpoint-epoch60.pth ...
2025-05-22 12:04:25,680 - train(rank0) - INFO - computing prototypes...
2025-05-22 12:04:27,538 - train(rank0) - INFO - [0/12]
2025-05-22 12:04:27,853 - train(rank0) - INFO - [2/12]
2025-05-22 12:04:28,097 - train(rank0) - INFO - [4/12]
2025-05-22 12:04:28,342 - train(rank0) - INFO - [6/12]
2025-05-22 12:04:28,586 - train(rank0) - INFO - [8/12]
2025-05-22 12:04:28,829 - train(rank0) - INFO - [10/12]
2025-05-22 12:04:29,301 - train(rank0) - INFO - computing noise...
2025-05-22 12:04:31,119 - train(rank0) - INFO - [0/12]
2025-05-22 12:04:31,382 - train(rank0) - INFO - [2/12]
2025-05-22 12:04:31,629 - train(rank0) - INFO - [4/12]
2025-05-22 12:04:31,875 - train(rank0) - INFO - [6/12]
2025-05-22 12:04:32,122 - train(rank0) - INFO - [8/12]
2025-05-22 12:04:32,369 - train(rank0) - INFO - [10/12]
2025-05-22 12:04:32,903 - train(rank0) - INFO - Epoch - 41
2025-05-22 12:04:35,509 - train(rank0) - INFO - lr[0]: 0.000037 / lr[1]: 0.000372 / lr[2]: 0.000372
2025-05-22 12:04:35,509 - train(rank0) - INFO - [0/12]
2025-05-22 12:04:36,702 - train(rank0) - INFO - [2/12]
2025-05-22 12:04:37,881 - train(rank0) - INFO - [4/12]
2025-05-22 12:04:39,059 - train(rank0) - INFO - [6/12]
2025-05-22 12:04:40,269 - train(rank0) - INFO - [8/12]
2025-05-22 12:04:41,455 - train(rank0) - INFO - [10/12]
2025-05-22 12:04:42,459 - train(rank0) - INFO -     epoch          : 41
2025-05-22 12:04:42,460 - train(rank0) - INFO -     loss           : 0.1622618151207765
2025-05-22 12:04:42,460 - train(rank0) - INFO -     loss_mbce      : 0.030857682538529236
2025-05-22 12:04:42,460 - train(rank0) - INFO -     loss_pkd       : 0.004253625168833726
2025-05-22 12:04:42,460 - train(rank0) - INFO -     loss_cont      : 0.06002040803432465
2025-05-22 12:04:42,461 - train(rank0) - INFO -     loss_uncer     : 0.06713009526332221
2025-05-22 12:04:42,477 - train(rank0) - INFO - Epoch - 42
2025-05-22 12:04:45,083 - train(rank0) - INFO - lr[0]: 0.000036 / lr[1]: 0.000355 / lr[2]: 0.000355
2025-05-22 12:04:45,084 - train(rank0) - INFO - [0/12]
2025-05-22 12:04:46,300 - train(rank0) - INFO - [2/12]
2025-05-22 12:04:47,499 - train(rank0) - INFO - [4/12]
2025-05-22 12:04:48,653 - train(rank0) - INFO - [6/12]
2025-05-22 12:04:49,824 - train(rank0) - INFO - [8/12]
2025-05-22 12:04:50,990 - train(rank0) - INFO - [10/12]
2025-05-22 12:04:52,003 - train(rank0) - INFO -     epoch          : 42
2025-05-22 12:04:52,004 - train(rank0) - INFO -     loss           : 0.15884056563178697
2025-05-22 12:04:52,004 - train(rank0) - INFO -     loss_mbce      : 0.024627114335695904
2025-05-22 12:04:52,004 - train(rank0) - INFO -     loss_pkd       : 0.00486234603158664
2025-05-22 12:04:52,004 - train(rank0) - INFO -     loss_cont      : 0.060439894596735634
2025-05-22 12:04:52,004 - train(rank0) - INFO -     loss_uncer     : 0.06891120821237565
2025-05-22 12:04:52,009 - train(rank0) - INFO - Epoch - 43
2025-05-22 12:04:54,708 - train(rank0) - INFO - lr[0]: 0.000034 / lr[1]: 0.000338 / lr[2]: 0.000338
2025-05-22 12:04:54,708 - train(rank0) - INFO - [0/12]
2025-05-22 12:04:55,943 - train(rank0) - INFO - [2/12]
2025-05-22 12:04:57,117 - train(rank0) - INFO - [4/12]
2025-05-22 12:04:58,273 - train(rank0) - INFO - [6/12]
2025-05-22 12:04:59,469 - train(rank0) - INFO - [8/12]
2025-05-22 12:05:00,693 - train(rank0) - INFO - [10/12]
2025-05-22 12:05:01,673 - train(rank0) - INFO -     epoch          : 43
2025-05-22 12:05:01,673 - train(rank0) - INFO -     loss           : 0.1687338761985302
2025-05-22 12:05:01,674 - train(rank0) - INFO -     loss_mbce      : 0.02949295697423319
2025-05-22 12:05:01,674 - train(rank0) - INFO -     loss_pkd       : 0.005239984614793987
2025-05-22 12:05:01,674 - train(rank0) - INFO -     loss_cont      : 0.06166885147492091
2025-05-22 12:05:01,674 - train(rank0) - INFO -     loss_uncer     : 0.07233208169539769
2025-05-22 12:05:01,760 - train(rank0) - INFO - Epoch - 44
2025-05-22 12:05:04,446 - train(rank0) - INFO - lr[0]: 0.000032 / lr[1]: 0.000321 / lr[2]: 0.000321
2025-05-22 12:05:04,447 - train(rank0) - INFO - [0/12]
2025-05-22 12:05:05,661 - train(rank0) - INFO - [2/12]
2025-05-22 12:05:06,873 - train(rank0) - INFO - [4/12]
2025-05-22 12:05:08,092 - train(rank0) - INFO - [6/12]
2025-05-22 12:05:09,269 - train(rank0) - INFO - [8/12]
2025-05-22 12:05:10,448 - train(rank0) - INFO - [10/12]
2025-05-22 12:05:11,496 - train(rank0) - INFO -     epoch          : 44
2025-05-22 12:05:11,497 - train(rank0) - INFO -     loss           : 0.1659577265381813
2025-05-22 12:05:11,497 - train(rank0) - INFO -     loss_mbce      : 0.02573261126720657
2025-05-22 12:05:11,497 - train(rank0) - INFO -     loss_pkd       : 0.006728367649581439
2025-05-22 12:05:11,497 - train(rank0) - INFO -     loss_cont      : 0.060702393949031826
2025-05-22 12:05:11,498 - train(rank0) - INFO -     loss_uncer     : 0.07279435098171234
2025-05-22 12:05:11,510 - train(rank0) - INFO - Epoch - 45
2025-05-22 12:05:14,652 - train(rank0) - INFO - lr[0]: 0.000030 / lr[1]: 0.000304 / lr[2]: 0.000304
2025-05-22 12:05:14,652 - train(rank0) - INFO - [0/12]
2025-05-22 12:05:15,817 - train(rank0) - INFO - [2/12]
2025-05-22 12:05:17,015 - train(rank0) - INFO - [4/12]
2025-05-22 12:05:18,230 - train(rank0) - INFO - [6/12]
2025-05-22 12:05:19,440 - train(rank0) - INFO - [8/12]
2025-05-22 12:05:20,597 - train(rank0) - INFO - [10/12]
2025-05-22 12:05:21,581 - train(rank0) - INFO -     epoch          : 45
2025-05-22 12:05:21,582 - train(rank0) - INFO -     loss           : 0.16988757128516832
2025-05-22 12:05:21,582 - train(rank0) - INFO -     loss_mbce      : 0.029953091715772946
2025-05-22 12:05:21,582 - train(rank0) - INFO -     loss_pkd       : 0.008203899463599859
2025-05-22 12:05:21,583 - train(rank0) - INFO -     loss_cont      : 0.06323967476685842
2025-05-22 12:05:21,583 - train(rank0) - INFO -     loss_uncer     : 0.06849090705315274
2025-05-22 12:05:21,605 - train(rank0) - INFO - Epoch - 46
2025-05-22 12:05:24,328 - train(rank0) - INFO - lr[0]: 0.000029 / lr[1]: 0.000287 / lr[2]: 0.000287
2025-05-22 12:05:24,328 - train(rank0) - INFO - [0/12]
2025-05-22 12:05:25,510 - train(rank0) - INFO - [2/12]
2025-05-22 12:05:26,655 - train(rank0) - INFO - [4/12]
2025-05-22 12:05:27,861 - train(rank0) - INFO - [6/12]
2025-05-22 12:05:29,026 - train(rank0) - INFO - [8/12]
2025-05-22 12:05:30,235 - train(rank0) - INFO - [10/12]
2025-05-22 12:05:31,334 - train(rank0) - INFO -     epoch          : 46
2025-05-22 12:05:31,334 - train(rank0) - INFO -     loss           : 0.16399330894152322
2025-05-22 12:05:31,335 - train(rank0) - INFO -     loss_mbce      : 0.028311588413392503
2025-05-22 12:05:31,335 - train(rank0) - INFO -     loss_pkd       : 0.005689235537526353
2025-05-22 12:05:31,335 - train(rank0) - INFO -     loss_cont      : 0.0638000379006068
2025-05-22 12:05:31,335 - train(rank0) - INFO -     loss_uncer     : 0.06619244664907455
2025-05-22 12:05:31,358 - train(rank0) - INFO - Epoch - 47
2025-05-22 12:05:34,113 - train(rank0) - INFO - lr[0]: 0.000027 / lr[1]: 0.000270 / lr[2]: 0.000270
2025-05-22 12:05:34,113 - train(rank0) - INFO - [0/12]
2025-05-22 12:05:35,293 - train(rank0) - INFO - [2/12]
2025-05-22 12:05:36,479 - train(rank0) - INFO - [4/12]
2025-05-22 12:05:37,613 - train(rank0) - INFO - [6/12]
2025-05-22 12:05:38,831 - train(rank0) - INFO - [8/12]
2025-05-22 12:05:40,042 - train(rank0) - INFO - [10/12]
2025-05-22 12:05:41,002 - train(rank0) - INFO -     epoch          : 47
2025-05-22 12:05:41,003 - train(rank0) - INFO -     loss           : 0.16699341560403505
2025-05-22 12:05:41,004 - train(rank0) - INFO -     loss_mbce      : 0.02608097018674016
2025-05-22 12:05:41,004 - train(rank0) - INFO -     loss_pkd       : 0.005377540001063608
2025-05-22 12:05:41,005 - train(rank0) - INFO -     loss_cont      : 0.06275309969981512
2025-05-22 12:05:41,005 - train(rank0) - INFO -     loss_uncer     : 0.07278180321057637
2025-05-22 12:05:41,020 - train(rank0) - INFO - Epoch - 48
2025-05-22 12:05:43,589 - train(rank0) - INFO - lr[0]: 0.000025 / lr[1]: 0.000252 / lr[2]: 0.000252
2025-05-22 12:05:43,589 - train(rank0) - INFO - [0/12]
2025-05-22 12:05:44,778 - train(rank0) - INFO - [2/12]
2025-05-22 12:05:45,948 - train(rank0) - INFO - [4/12]
2025-05-22 12:05:47,136 - train(rank0) - INFO - [6/12]
2025-05-22 12:05:48,318 - train(rank0) - INFO - [8/12]
2025-05-22 12:05:49,511 - train(rank0) - INFO - [10/12]
2025-05-22 12:05:50,525 - train(rank0) - INFO -     epoch          : 48
2025-05-22 12:05:50,526 - train(rank0) - INFO -     loss           : 0.16520981738964716
2025-05-22 12:05:50,526 - train(rank0) - INFO -     loss_mbce      : 0.028244747935483854
2025-05-22 12:05:50,526 - train(rank0) - INFO -     loss_pkd       : 0.004688682250465111
2025-05-22 12:05:50,527 - train(rank0) - INFO -     loss_cont      : 0.06194645464420318
2025-05-22 12:05:50,527 - train(rank0) - INFO -     loss_uncer     : 0.07032992988824845
2025-05-22 12:05:50,540 - train(rank0) - INFO - Epoch - 49
2025-05-22 12:05:53,325 - train(rank0) - INFO - lr[0]: 0.000023 / lr[1]: 0.000235 / lr[2]: 0.000235
2025-05-22 12:05:53,325 - train(rank0) - INFO - [0/12]
2025-05-22 12:05:54,350 - train(rank0) - INFO - [2/12]
2025-05-22 12:05:55,559 - train(rank0) - INFO - [4/12]
2025-05-22 12:05:56,763 - train(rank0) - INFO - [6/12]
2025-05-22 12:05:57,963 - train(rank0) - INFO - [8/12]
2025-05-22 12:05:59,172 - train(rank0) - INFO - [10/12]
2025-05-22 12:06:00,189 - train(rank0) - INFO -     epoch          : 49
2025-05-22 12:06:00,190 - train(rank0) - INFO -     loss           : 0.1617525207499663
2025-05-22 12:06:00,191 - train(rank0) - INFO -     loss_mbce      : 0.024362290588517983
2025-05-22 12:06:00,191 - train(rank0) - INFO -     loss_pkd       : 0.006685946027573664
2025-05-22 12:06:00,191 - train(rank0) - INFO -     loss_cont      : 0.05816811472177505
2025-05-22 12:06:00,191 - train(rank0) - INFO -     loss_uncer     : 0.07253616799910864
2025-05-22 12:06:00,200 - train(rank0) - INFO - Epoch - 50
2025-05-22 12:06:02,886 - train(rank0) - INFO - lr[0]: 0.000022 / lr[1]: 0.000217 / lr[2]: 0.000217
2025-05-22 12:06:02,886 - train(rank0) - INFO - [0/12]
2025-05-22 12:06:04,080 - train(rank0) - INFO - [2/12]
2025-05-22 12:06:05,261 - train(rank0) - INFO - [4/12]
2025-05-22 12:06:06,453 - train(rank0) - INFO - [6/12]
2025-05-22 12:06:07,636 - train(rank0) - INFO - [8/12]
2025-05-22 12:06:08,858 - train(rank0) - INFO - [10/12]
2025-05-22 12:06:09,898 - train(rank0) - INFO - Number of val loader: 57
2025-05-22 12:06:13,353 - train(rank0) - INFO -     epoch          : 50
2025-05-22 12:06:13,354 - train(rank0) - INFO -     loss           : 0.16285299758116403
2025-05-22 12:06:13,354 - train(rank0) - INFO -     loss_mbce      : 0.02726537020256122
2025-05-22 12:06:13,354 - train(rank0) - INFO -     loss_pkd       : 0.006590108387172222
2025-05-22 12:06:13,354 - train(rank0) - INFO -     loss_cont      : 0.059974650045235944
2025-05-22 12:06:13,354 - train(rank0) - INFO -     loss_uncer     : 0.06902286261320116
2025-05-22 12:06:13,354 - train(rank0) - INFO -     val_Pixel_Accuracy_old: 98.55
2025-05-22 12:06:13,354 - train(rank0) - INFO -     val_Pixel_Accuracy_new: 68.86
2025-05-22 12:06:13,354 - train(rank0) - INFO -     val_Pixel_Accuracy_harmonic: 81.07
2025-05-22 12:06:13,355 - train(rank0) - INFO -     val_Pixel_Accuracy_overall: 91.64
2025-05-22 12:06:13,355 - train(rank0) - INFO -     val_Pixel_Accuracy_Class_old: 98.55
2025-05-22 12:06:13,355 - train(rank0) - INFO -     val_Pixel_Accuracy_Class_new: 68.86
2025-05-22 12:06:13,355 - train(rank0) - INFO -     val_Pixel_Accuracy_Class_harmonic: 81.07
2025-05-22 12:06:13,355 - train(rank0) - INFO -     val_Pixel_Accuracy_Class_overall: 83.71
2025-05-22 12:06:13,355 - train(rank0) - INFO -     val_Pixel_Accuracy_Class_by_class: 
 0  background 98.55
17 *sheep 68.86

2025-05-22 12:06:13,355 - train(rank0) - INFO -     val_Mean_Intersection_over_Union_old: 94.00
2025-05-22 12:06:13,355 - train(rank0) - INFO -     val_Mean_Intersection_over_Union_new: 68.64
2025-05-22 12:06:13,355 - train(rank0) - INFO -     val_Mean_Intersection_over_Union_harmonic: 79.34
2025-05-22 12:06:13,355 - train(rank0) - INFO -     val_Mean_Intersection_over_Union_overall: 81.32
2025-05-22 12:06:13,355 - train(rank0) - INFO -     val_Mean_Intersection_over_Union_by_class: 
 0  background 94.00
17 *sheep 68.64

2025-05-22 12:06:14,050 - train(rank0) - INFO - Saving checkpoint: saved_voc/models/overlap_15-1_Adapter/step_2/checkpoint-epoch60.pth ...
2025-05-22 12:06:14,057 - train(rank0) - INFO - computing prototypes...
2025-05-22 12:06:15,903 - train(rank0) - INFO - [0/12]
2025-05-22 12:06:16,146 - train(rank0) - INFO - [2/12]
2025-05-22 12:06:16,389 - train(rank0) - INFO - [4/12]
2025-05-22 12:06:16,634 - train(rank0) - INFO - [6/12]
2025-05-22 12:06:16,877 - train(rank0) - INFO - [8/12]
2025-05-22 12:06:17,121 - train(rank0) - INFO - [10/12]
2025-05-22 12:06:17,608 - train(rank0) - INFO - computing noise...
2025-05-22 12:06:19,497 - train(rank0) - INFO - [0/12]
2025-05-22 12:06:19,746 - train(rank0) - INFO - [2/12]
2025-05-22 12:06:19,993 - train(rank0) - INFO - [4/12]
2025-05-22 12:06:20,240 - train(rank0) - INFO - [6/12]
2025-05-22 12:06:20,486 - train(rank0) - INFO - [8/12]
2025-05-22 12:06:20,733 - train(rank0) - INFO - [10/12]
2025-05-22 12:06:21,295 - train(rank0) - INFO - Epoch - 51
2025-05-22 12:06:23,916 - train(rank0) - INFO - lr[0]: 0.000020 / lr[1]: 0.000199 / lr[2]: 0.000199
2025-05-22 12:06:23,916 - train(rank0) - INFO - [0/12]
2025-05-22 12:06:25,124 - train(rank0) - INFO - [2/12]
2025-05-22 12:06:26,352 - train(rank0) - INFO - [4/12]
2025-05-22 12:06:27,552 - train(rank0) - INFO - [6/12]
2025-05-22 12:06:28,697 - train(rank0) - INFO - [8/12]
2025-05-22 12:06:29,916 - train(rank0) - INFO - [10/12]
2025-05-22 12:06:30,834 - train(rank0) - INFO -     epoch          : 51
2025-05-22 12:06:30,835 - train(rank0) - INFO -     loss           : 0.16666407386461893
2025-05-22 12:06:30,835 - train(rank0) - INFO -     loss_mbce      : 0.03190469245115916
2025-05-22 12:06:30,835 - train(rank0) - INFO -     loss_pkd       : 0.005216375126716836
2025-05-22 12:06:30,835 - train(rank0) - INFO -     loss_cont      : 0.05830828398466111
2025-05-22 12:06:30,836 - train(rank0) - INFO -     loss_uncer     : 0.07123472342888515
2025-05-22 12:06:30,889 - train(rank0) - INFO - Epoch - 52
2025-05-22 12:06:33,532 - train(rank0) - INFO - lr[0]: 0.000018 / lr[1]: 0.000181 / lr[2]: 0.000181
2025-05-22 12:06:33,532 - train(rank0) - INFO - [0/12]
2025-05-22 12:06:34,726 - train(rank0) - INFO - [2/12]
2025-05-22 12:06:35,938 - train(rank0) - INFO - [4/12]
2025-05-22 12:06:36,932 - train(rank0) - INFO - [6/12]
2025-05-22 12:06:38,144 - train(rank0) - INFO - [8/12]
2025-05-22 12:06:39,280 - train(rank0) - INFO - [10/12]
2025-05-22 12:06:40,334 - train(rank0) - INFO -     epoch          : 52
2025-05-22 12:06:40,335 - train(rank0) - INFO -     loss           : 0.1611198236544927
2025-05-22 12:06:40,335 - train(rank0) - INFO -     loss_mbce      : 0.02616846716652314
2025-05-22 12:06:40,335 - train(rank0) - INFO -     loss_pkd       : 0.006252653583942447
2025-05-22 12:06:40,336 - train(rank0) - INFO -     loss_cont      : 0.05881433685620626
2025-05-22 12:06:40,336 - train(rank0) - INFO -     loss_uncer     : 0.06988436182339987
2025-05-22 12:06:40,345 - train(rank0) - INFO - Epoch - 53
2025-05-22 12:06:43,009 - train(rank0) - INFO - lr[0]: 0.000016 / lr[1]: 0.000163 / lr[2]: 0.000163
2025-05-22 12:06:43,009 - train(rank0) - INFO - [0/12]
2025-05-22 12:06:44,073 - train(rank0) - INFO - [2/12]
2025-05-22 12:06:45,234 - train(rank0) - INFO - [4/12]
2025-05-22 12:06:46,426 - train(rank0) - INFO - [6/12]
2025-05-22 12:06:47,608 - train(rank0) - INFO - [8/12]
2025-05-22 12:06:48,806 - train(rank0) - INFO - [10/12]
2025-05-22 12:06:49,838 - train(rank0) - INFO -     epoch          : 53
2025-05-22 12:06:49,839 - train(rank0) - INFO -     loss           : 0.1614879568417867
2025-05-22 12:06:49,840 - train(rank0) - INFO -     loss_mbce      : 0.03105350040520231
2025-05-22 12:06:49,840 - train(rank0) - INFO -     loss_pkd       : 0.006571503193602742
2025-05-22 12:06:49,840 - train(rank0) - INFO -     loss_cont      : 0.05877187152703603
2025-05-22 12:06:49,840 - train(rank0) - INFO -     loss_uncer     : 0.06509107748667399
2025-05-22 12:06:49,852 - train(rank0) - INFO - Epoch - 54
2025-05-22 12:06:52,431 - train(rank0) - INFO - lr[0]: 0.000014 / lr[1]: 0.000145 / lr[2]: 0.000145
2025-05-22 12:06:52,432 - train(rank0) - INFO - [0/12]
2025-05-22 12:06:53,611 - train(rank0) - INFO - [2/12]
2025-05-22 12:06:54,792 - train(rank0) - INFO - [4/12]
2025-05-22 12:06:55,972 - train(rank0) - INFO - [6/12]
2025-05-22 12:06:57,185 - train(rank0) - INFO - [8/12]
2025-05-22 12:06:58,387 - train(rank0) - INFO - [10/12]
2025-05-22 12:06:59,408 - train(rank0) - INFO -     epoch          : 54
2025-05-22 12:06:59,409 - train(rank0) - INFO -     loss           : 0.16087432826558748
2025-05-22 12:06:59,409 - train(rank0) - INFO -     loss_mbce      : 0.028988713631406426
2025-05-22 12:06:59,410 - train(rank0) - INFO -     loss_pkd       : 0.0038608492347217784
2025-05-22 12:06:59,410 - train(rank0) - INFO -     loss_cont      : 0.06157108892997105
2025-05-22 12:06:59,410 - train(rank0) - INFO -     loss_uncer     : 0.06645367493232092
2025-05-22 12:06:59,419 - train(rank0) - INFO - Epoch - 55
2025-05-22 12:07:02,106 - train(rank0) - INFO - lr[0]: 0.000013 / lr[1]: 0.000126 / lr[2]: 0.000126
2025-05-22 12:07:02,106 - train(rank0) - INFO - [0/12]
2025-05-22 12:07:03,291 - train(rank0) - INFO - [2/12]
2025-05-22 12:07:04,522 - train(rank0) - INFO - [4/12]
2025-05-22 12:07:05,677 - train(rank0) - INFO - [6/12]
2025-05-22 12:07:06,849 - train(rank0) - INFO - [8/12]
2025-05-22 12:07:08,032 - train(rank0) - INFO - [10/12]
2025-05-22 12:07:09,010 - train(rank0) - INFO -     epoch          : 55
2025-05-22 12:07:09,011 - train(rank0) - INFO -     loss           : 0.16161270315448442
2025-05-22 12:07:09,012 - train(rank0) - INFO -     loss_mbce      : 0.02677378539616863
2025-05-22 12:07:09,012 - train(rank0) - INFO -     loss_pkd       : 0.0036056636539190854
2025-05-22 12:07:09,012 - train(rank0) - INFO -     loss_cont      : 0.05977231413125992
2025-05-22 12:07:09,012 - train(rank0) - INFO -     loss_uncer     : 0.07146093845367432
2025-05-22 12:07:09,118 - train(rank0) - INFO - Epoch - 56
2025-05-22 12:07:11,702 - train(rank0) - INFO - lr[0]: 0.000011 / lr[1]: 0.000107 / lr[2]: 0.000107
2025-05-22 12:07:11,702 - train(rank0) - INFO - [0/12]
2025-05-22 12:07:12,933 - train(rank0) - INFO - [2/12]
2025-05-22 12:07:14,109 - train(rank0) - INFO - [4/12]
2025-05-22 12:07:15,302 - train(rank0) - INFO - [6/12]
2025-05-22 12:07:16,501 - train(rank0) - INFO - [8/12]
2025-05-22 12:07:17,670 - train(rank0) - INFO - [10/12]
2025-05-22 12:07:18,714 - train(rank0) - INFO -     epoch          : 56
2025-05-22 12:07:18,715 - train(rank0) - INFO -     loss           : 0.1770723176499208
2025-05-22 12:07:18,716 - train(rank0) - INFO -     loss_mbce      : 0.02775244740769267
2025-05-22 12:07:18,716 - train(rank0) - INFO -     loss_pkd       : 0.016177996488598485
2025-05-22 12:07:18,716 - train(rank0) - INFO -     loss_cont      : 0.06014155596494675
2025-05-22 12:07:18,716 - train(rank0) - INFO -     loss_uncer     : 0.07300031185150146
2025-05-22 12:07:18,726 - train(rank0) - INFO - Epoch - 57
2025-05-22 12:07:21,339 - train(rank0) - INFO - lr[0]: 0.000009 / lr[1]: 0.000087 / lr[2]: 0.000087
2025-05-22 12:07:21,340 - train(rank0) - INFO - [0/12]
2025-05-22 12:07:22,524 - train(rank0) - INFO - [2/12]
2025-05-22 12:07:23,557 - train(rank0) - INFO - [4/12]
2025-05-22 12:07:24,716 - train(rank0) - INFO - [6/12]
2025-05-22 12:07:25,898 - train(rank0) - INFO - [8/12]
2025-05-22 12:07:27,082 - train(rank0) - INFO - [10/12]
2025-05-22 12:07:28,016 - train(rank0) - INFO -     epoch          : 57
2025-05-22 12:07:28,017 - train(rank0) - INFO -     loss           : 0.16144335269927979
2025-05-22 12:07:28,017 - train(rank0) - INFO -     loss_mbce      : 0.026640487757200997
2025-05-22 12:07:28,017 - train(rank0) - INFO -     loss_pkd       : 0.006868059863336384
2025-05-22 12:07:28,017 - train(rank0) - INFO -     loss_cont      : 0.05967270880937577
2025-05-22 12:07:28,017 - train(rank0) - INFO -     loss_uncer     : 0.06826209276914598
2025-05-22 12:07:28,075 - train(rank0) - INFO - Epoch - 58
2025-05-22 12:07:30,667 - train(rank0) - INFO - lr[0]: 0.000007 / lr[1]: 0.000067 / lr[2]: 0.000067
2025-05-22 12:07:30,667 - train(rank0) - INFO - [0/12]
2025-05-22 12:07:31,836 - train(rank0) - INFO - [2/12]
2025-05-22 12:07:33,041 - train(rank0) - INFO - [4/12]
2025-05-22 12:07:34,227 - train(rank0) - INFO - [6/12]
2025-05-22 12:07:35,401 - train(rank0) - INFO - [8/12]
2025-05-22 12:07:36,592 - train(rank0) - INFO - [10/12]
2025-05-22 12:07:37,706 - train(rank0) - INFO -     epoch          : 58
2025-05-22 12:07:37,707 - train(rank0) - INFO -     loss           : 0.16259058813254038
2025-05-22 12:07:37,707 - train(rank0) - INFO -     loss_mbce      : 0.029539699433371425
2025-05-22 12:07:37,707 - train(rank0) - INFO -     loss_pkd       : 0.004224409373515907
2025-05-22 12:07:37,707 - train(rank0) - INFO -     loss_cont      : 0.05971642881631852
2025-05-22 12:07:37,707 - train(rank0) - INFO -     loss_uncer     : 0.06911004980405172
2025-05-22 12:07:37,720 - train(rank0) - INFO - Epoch - 59
2025-05-22 12:07:40,213 - train(rank0) - INFO - lr[0]: 0.000005 / lr[1]: 0.000047 / lr[2]: 0.000047
2025-05-22 12:07:40,213 - train(rank0) - INFO - [0/12]
2025-05-22 12:07:41,337 - train(rank0) - INFO - [2/12]
2025-05-22 12:07:42,489 - train(rank0) - INFO - [4/12]
2025-05-22 12:07:43,671 - train(rank0) - INFO - [6/12]
2025-05-22 12:07:44,855 - train(rank0) - INFO - [8/12]
2025-05-22 12:07:46,039 - train(rank0) - INFO - [10/12]
2025-05-22 12:07:47,033 - train(rank0) - INFO -     epoch          : 59
2025-05-22 12:07:47,034 - train(rank0) - INFO -     loss           : 0.16084270303448042
2025-05-22 12:07:47,034 - train(rank0) - INFO -     loss_mbce      : 0.02394864708185196
2025-05-22 12:07:47,035 - train(rank0) - INFO -     loss_pkd       : 0.005151839031896088
2025-05-22 12:07:47,035 - train(rank0) - INFO -     loss_cont      : 0.0603694126009941
2025-05-22 12:07:47,035 - train(rank0) - INFO -     loss_uncer     : 0.07137280255556107
2025-05-22 12:07:47,105 - train(rank0) - INFO - Epoch - 60
2025-05-22 12:07:49,921 - train(rank0) - INFO - lr[0]: 0.000003 / lr[1]: 0.000025 / lr[2]: 0.000025
2025-05-22 12:07:49,921 - train(rank0) - INFO - [0/12]
2025-05-22 12:07:51,072 - train(rank0) - INFO - [2/12]
2025-05-22 12:07:52,266 - train(rank0) - INFO - [4/12]
2025-05-22 12:07:53,459 - train(rank0) - INFO - [6/12]
2025-05-22 12:07:54,653 - train(rank0) - INFO - [8/12]
2025-05-22 12:07:55,815 - train(rank0) - INFO - [10/12]
2025-05-22 12:07:56,851 - train(rank0) - INFO - Number of val loader: 57
2025-05-22 12:08:00,236 - train(rank0) - INFO -     epoch          : 60
2025-05-22 12:08:00,237 - train(rank0) - INFO -     loss           : 0.16178744286298752
2025-05-22 12:08:00,237 - train(rank0) - INFO -     loss_mbce      : 0.025278370827436447
2025-05-22 12:08:00,237 - train(rank0) - INFO -     loss_pkd       : 0.0042450382534298114
2025-05-22 12:08:00,237 - train(rank0) - INFO -     loss_cont      : 0.061671327054500576
2025-05-22 12:08:00,237 - train(rank0) - INFO -     loss_uncer     : 0.07059270391861598
2025-05-22 12:08:00,237 - train(rank0) - INFO -     val_Pixel_Accuracy_old: 98.55
2025-05-22 12:08:00,237 - train(rank0) - INFO -     val_Pixel_Accuracy_new: 67.81
2025-05-22 12:08:00,238 - train(rank0) - INFO -     val_Pixel_Accuracy_harmonic: 80.34
2025-05-22 12:08:00,238 - train(rank0) - INFO -     val_Pixel_Accuracy_overall: 91.40
2025-05-22 12:08:00,238 - train(rank0) - INFO -     val_Pixel_Accuracy_Class_old: 98.55
2025-05-22 12:08:00,238 - train(rank0) - INFO -     val_Pixel_Accuracy_Class_new: 67.81
2025-05-22 12:08:00,238 - train(rank0) - INFO -     val_Pixel_Accuracy_Class_harmonic: 80.34
2025-05-22 12:08:00,238 - train(rank0) - INFO -     val_Pixel_Accuracy_Class_overall: 83.18
2025-05-22 12:08:00,238 - train(rank0) - INFO -     val_Pixel_Accuracy_Class_by_class: 
 0  background 98.55
17 *sheep 67.81

2025-05-22 12:08:00,238 - train(rank0) - INFO -     val_Mean_Intersection_over_Union_old: 93.88
2025-05-22 12:08:00,239 - train(rank0) - INFO -     val_Mean_Intersection_over_Union_new: 67.61
2025-05-22 12:08:00,239 - train(rank0) - INFO -     val_Mean_Intersection_over_Union_harmonic: 78.61
2025-05-22 12:08:00,239 - train(rank0) - INFO -     val_Mean_Intersection_over_Union_overall: 80.75
2025-05-22 12:08:00,239 - train(rank0) - INFO -     val_Mean_Intersection_over_Union_by_class: 
 0  background 93.88
17 *sheep 67.61

2025-05-22 12:08:00,239 - train(rank0) - INFO - Validation performance didn't improve for 60 epochs. Training stops.
2025-05-22 12:08:00,298 - train(rank0) - INFO - Number of test loader: 1329
